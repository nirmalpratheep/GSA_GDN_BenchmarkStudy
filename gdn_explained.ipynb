{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated DeltaNet (GDN) — Step-by-Step Explanation\n",
    "\n",
    "**Reference:** [NVlabs/GatedDeltaNet](https://github.com/NVlabs/GatedDeltaNet)\n",
    "\n",
    "This notebook walks through the entire Gated DeltaNet forward pass using a **tiny example** so you can inspect every tensor at every step.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Input x [B, T, D]\n",
    "  │\n",
    "  ├──► q_proj ──► q_conv1d ──► reshape ──► RoPE ──► L2 norm ──► scale ──┐\n",
    "  ├──► k_proj ──► k_conv1d ──► reshape ──► RoPE ──► L2 norm ──────────►│\n",
    "  ├──► v_proj ──► v_conv1d ──► reshape ────────────────────────────────►│ Recurrent\n",
    "  ├──► b_proj ──► sigmoid (write gate β) ──────────────────────────────►│ Delta Rule\n",
    "  ├──► gk_proj ──► Mamba gate (-A.exp() * softplus(gk + dt_bias)) ────►│\n",
    "  │                                                                     │\n",
    "  │                                              output o ◄─────────────┘\n",
    "  │                                                │\n",
    "  ├──► g_proj ────────────────────────────────► FusedRMSNormSwishGate\n",
    "  │                                                │\n",
    "  │                                           o_proj ──► Output\n",
    "```\n",
    "\n",
    "### Key Idea: The Delta Rule\n",
    "\n",
    "GDN maintains a **recurrent state matrix** `S` of shape `[d_k, d_v]` per head. At each time step:\n",
    "\n",
    "1. **Decay**: `S = S * exp(g)` — forget old information\n",
    "2. **Delta correction**: `v_new = (v - S @ k) * β` — compute what's *new* relative to current memory\n",
    "3. **Update**: `S = S + k ⊗ v_new` — write the new information (rank-1 outer product)\n",
    "4. **Read**: `o = q @ S` — query the memory\n",
    "\n",
    "This is a **linear-time** recurrence (O(T)) compared to O(T²) for standard attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Tiny example dimensions\n",
    "B = 1       # batch size\n",
    "T = 6       # sequence length (small enough to inspect)\n",
    "D = 32      # model dimension\n",
    "H = 2       # number of heads\n",
    "expand_k = 0.75  # key expansion factor (ref default)\n",
    "expand_v = 1.5   # value expansion factor (ref default)\n",
    "\n",
    "key_dim = int(D * expand_k)    # 24\n",
    "value_dim = int(D * expand_v)  # 48\n",
    "d_k = key_dim // H             # 12 (per-head key dim)\n",
    "d_v = value_dim // H           # 24 (per-head value dim)\n",
    "\n",
    "print(f\"Model dim D={D}, Heads H={H}\")\n",
    "print(f\"Key dim: {key_dim} (expand_k={expand_k}), per head: {d_k}\")\n",
    "print(f\"Value dim: {value_dim} (expand_v={expand_v}), per head: {d_v}\")\n",
    "print(f\"State matrix S per head: [{d_k} x {d_v}] = {d_k * d_v} parameters\")\n",
    "print(f\"\\nNote: key_dim != value_dim. This asymmetry is a key design choice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Input and Projections\n",
    "\n",
    "The input `x` is projected into **Q, K, V** with different output dimensions:\n",
    "- Q, K → `key_dim` (D × expand_k = 24)\n",
    "- V → `value_dim` (D × expand_v = 48)\n",
    "\n",
    "Additionally, we project **two scalar gates per head**:\n",
    "- `β` (write gate) via `b_proj` → sigmoid → controls how much new info to write\n",
    "- `gk` (decay gate) via `gk_proj` → Mamba-style transform → controls forgetting rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple input\n",
    "x = torch.randn(B, T, D)\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "print(f\"Input x (first 2 tokens, first 8 dims):\\n{x[0, :2, :8]}\")\n",
    "\n",
    "# Linear projections\n",
    "q_proj = nn.Linear(D, key_dim, bias=False)\n",
    "k_proj = nn.Linear(D, key_dim, bias=False)\n",
    "v_proj = nn.Linear(D, value_dim, bias=False)\n",
    "b_proj = nn.Linear(D, H, bias=True)     # write gate\n",
    "gk_proj = nn.Linear(D, H, bias=False)   # decay gate (no bias when use_mamba_gate=True)\n",
    "g_proj = nn.Linear(D, value_dim, bias=False)  # output gate projection\n",
    "\n",
    "q = q_proj(x)   # [B, T, key_dim=24]\n",
    "k = k_proj(x)   # [B, T, key_dim=24]\n",
    "v = v_proj(x)   # [B, T, value_dim=48]\n",
    "\n",
    "print(f\"\\nQ shape: {q.shape}  (key_dim={key_dim})\")\n",
    "print(f\"K shape: {k.shape}  (key_dim={key_dim})\")\n",
    "print(f\"V shape: {v.shape}  (value_dim={value_dim})\")\n",
    "print(f\"\\nNote: Q,K have dim {key_dim} but V has dim {value_dim} (asymmetric!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Short Convolutions\n",
    "\n",
    "Before the recurrence, Q, K, V pass through **depthwise 1D convolutions** (kernel=4) with SiLU activation.\n",
    "\n",
    "This allows local context mixing before the global recurrence. Think of it as a \"local receptive field\" preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short Convolution (simplified — depthwise conv1d + SiLU)\n",
    "conv_size = 4\n",
    "\n",
    "q_conv = nn.Conv1d(key_dim, key_dim, kernel_size=conv_size, padding=conv_size-1, groups=key_dim)\n",
    "k_conv = nn.Conv1d(key_dim, key_dim, kernel_size=conv_size, padding=conv_size-1, groups=key_dim)\n",
    "v_conv = nn.Conv1d(value_dim, value_dim, kernel_size=conv_size, padding=conv_size-1, groups=value_dim)\n",
    "\n",
    "def short_conv(conv, x_in, conv_size):\n",
    "    \"\"\"Apply causal depthwise conv + SiLU\"\"\"\n",
    "    out = conv(x_in.transpose(1, 2))        # [B, C, T+pad]\n",
    "    out = out[:, :, :-(conv_size - 1)]       # remove right padding for causal\n",
    "    return F.silu(out).transpose(1, 2)       # [B, T, C]\n",
    "\n",
    "q = short_conv(q_conv, q, conv_size)\n",
    "k = short_conv(k_conv, k, conv_size)\n",
    "v = short_conv(v_conv, v, conv_size)\n",
    "\n",
    "print(f\"After short conv + SiLU:\")\n",
    "print(f\"  Q shape: {q.shape}\")\n",
    "print(f\"  K shape: {k.shape}\")\n",
    "print(f\"  V shape: {v.shape}\")\n",
    "print(f\"\\nConv is causal (only looks at current + past {conv_size-1} positions)\")\n",
    "print(f\"SiLU activation: x * sigmoid(x) — smooth non-linearity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reshape to Multi-Head Format\n",
    "\n",
    "Reshape Q, K, V into multi-head format: `[B, T, H, d_per_head]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.view(B, T, H, d_k)   # [1, 6, 2, 12]\n",
    "k = k.view(B, T, H, d_k)   # [1, 6, 2, 12]\n",
    "v = v.view(B, T, H, d_v)   # [1, 6, 2, 24]\n",
    "\n",
    "print(f\"Multi-head shapes:\")\n",
    "print(f\"  Q: {q.shape}  — H={H} heads, d_k={d_k} per head\")\n",
    "print(f\"  K: {k.shape}  — H={H} heads, d_k={d_k} per head\")\n",
    "print(f\"  V: {v.shape}  — H={H} heads, d_v={d_v} per head\")\n",
    "print(f\"\\nQ,K heads have dimension {d_k} but V heads have dimension {d_v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Rotary Position Embeddings (RoPE)\n",
    "\n",
    "RoPE encodes position information by **rotating** Q and K vectors in 2D subspaces.\n",
    "\n",
    "Formula: `x_rotated = x * cos(θ) + rotate_half(x) * sin(θ)`\n",
    "\n",
    "Where `rotate_half` splits the vector in half and swaps with negation: `[-x2, x1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RoPE frequencies\n",
    "base = 10000.0\n",
    "inv_freq = 1.0 / (base ** (torch.arange(0, d_k, 2).float() / d_k))\n",
    "print(f\"inv_freq (one per 2D rotation plane): {inv_freq}\")\n",
    "print(f\"Number of rotation planes: {len(inv_freq)} = d_k/2 = {d_k}//2\")\n",
    "\n",
    "# Position-dependent angles\n",
    "t = torch.arange(T, dtype=torch.float32)\n",
    "freqs = torch.outer(t, inv_freq)\n",
    "emb = torch.cat((freqs, freqs), dim=-1)  # [T, d_k]\n",
    "cos_cached = emb.cos().unsqueeze(0).unsqueeze(2)  # [1, T, 1, d_k]\n",
    "sin_cached = emb.sin().unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "print(f\"\\nAngles for each position (first 3 planes):\")\n",
    "for pos in range(min(4, T)):\n",
    "    angles = freqs[pos, :3]\n",
    "    print(f\"  pos={pos}: {angles.numpy()} radians\")\n",
    "\n",
    "# Apply RoPE\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., :x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "q_before_rope = q.clone()\n",
    "q = q * cos_cached + rotate_half(q) * sin_cached\n",
    "k = k * cos_cached + rotate_half(k) * sin_cached\n",
    "\n",
    "print(f\"\\nQ before RoPE (head 0, token 0, first 6 dims): {q_before_rope[0, 0, 0, :6].numpy().round(3)}\")\n",
    "print(f\"Q after  RoPE (head 0, token 0, first 6 dims): {q[0, 0, 0, :6].numpy().round(3)}\")\n",
    "print(f\"\\nRoPE makes dot(q_i, k_j) depend on relative position (i-j)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: L2 Normalization\n",
    "\n",
    "Q and K are L2-normalized so that their dot products are bounded in [-1, 1].\n",
    "\n",
    "This stabilizes the recurrence — without normalization, the state matrix `S` can grow unboundedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_before_norm = q.clone()\n",
    "q = F.normalize(q, p=2, dim=-1)\n",
    "k = F.normalize(k, p=2, dim=-1)\n",
    "\n",
    "print(f\"Before L2 norm — Q norms per head, token:\")\n",
    "print(f\"  {q_before_norm[0, :, 0, :].norm(dim=-1).numpy().round(3)}\")\n",
    "\n",
    "print(f\"\\nAfter L2 norm — all norms = 1.0:\")\n",
    "print(f\"  {q[0, :, 0, :].norm(dim=-1).numpy().round(3)}\")\n",
    "\n",
    "print(f\"\\nThis bounds dot(q, k) ∈ [-1, 1] — critical for stable recurrence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Gate Computation\n",
    "\n",
    "Two gates control the recurrence:\n",
    "\n",
    "### Write Gate β (beta)\n",
    "- `β = sigmoid(b_proj(x))` — scalar per head ∈ (0, 1)\n",
    "- Controls how strongly new information is written to the state\n",
    "\n",
    "### Decay Gate g (Mamba-style)\n",
    "- `g = -A.exp() * softplus(gk_proj(x) + dt_bias)` — always **negative** (log-space decay)\n",
    "- `exp(g)` ∈ (0, 1) controls how much old state is retained\n",
    "- `-A.exp()` ensures decay is always positive (forgetting)\n",
    "- `softplus(...)` ensures the rate magnitude is always positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write gate β\n",
    "beta_raw = b_proj(x)  # [B, T, H]\n",
    "beta = beta_raw.float().sigmoid().transpose(1, 2)  # [B, H, T]\n",
    "\n",
    "print(\"Write gate β (sigmoid output per head, per token):\")\n",
    "print(f\"  Shape: {beta.shape} = [B, H, T]\")\n",
    "for h in range(H):\n",
    "    print(f\"  Head {h}: {beta[0, h].numpy().round(3)}\")\n",
    "\n",
    "# Decay gate g (Mamba-style)\n",
    "# Initialize A_log and dt_bias as in reference\n",
    "A_log = nn.Parameter(torch.log(torch.tensor([4.0, 8.0])))  # 2 heads\n",
    "dt_min, dt_max = 0.001, 0.1\n",
    "dt = torch.exp(torch.rand(H) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n",
    "dt_bias = nn.Parameter(dt + torch.log(-torch.expm1(-dt)))\n",
    "\n",
    "gk_raw = gk_proj(x).float()  # [B, T, H]\n",
    "A = A_log.float().exp()\n",
    "gk = -A * F.softplus(gk_raw + dt_bias)  # Always negative!\n",
    "gk = gk.transpose(1, 2)  # [B, H, T]\n",
    "\n",
    "print(f\"\\nDecay gate g (log-space, always negative):\")\n",
    "for h in range(H):\n",
    "    print(f\"  Head {h}: {gk[0, h].detach().numpy().round(4)}\")\n",
    "print(f\"\\nexp(g) = retention ratio:\")\n",
    "for h in range(H):\n",
    "    print(f\"  Head {h}: {gk[0, h].exp().detach().numpy().round(4)}\")\n",
    "print(f\"\\nValues near 1.0 = keep memory; near 0.0 = forget everything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Prepare for Recurrence\n",
    "\n",
    "Transpose to `[B, H, T, d]` format and apply query scaling by `d_k^(-0.5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose: [B, T, H, d] -> [B, H, T, d]\n",
    "q = q.transpose(1, 2)  # [B, H, T, d_k]\n",
    "k = k.transpose(1, 2)  # [B, H, T, d_k]\n",
    "v = v.transpose(1, 2)  # [B, H, T, d_v]\n",
    "\n",
    "# Scale queries\n",
    "scale = d_k ** -0.5\n",
    "q = q * scale\n",
    "\n",
    "print(f\"Shapes for recurrence:\")\n",
    "print(f\"  Q: {q.shape} (scaled by {scale:.4f} = 1/√{d_k})\")\n",
    "print(f\"  K: {k.shape}\")\n",
    "print(f\"  V: {v.shape}\")\n",
    "print(f\"  β: {beta.shape}\")\n",
    "print(f\"  g: {gk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: The Delta Rule Recurrence (Core Algorithm)\n",
    "\n",
    "This is the **heart of GDN**. We process tokens one by one, maintaining a state matrix `S`.\n",
    "\n",
    "For each time step `i`:\n",
    "\n",
    "```\n",
    "1. DECAY:   S = S * exp(g_i)                    # Forget old info\n",
    "2. RECALL:  recalled = (S * k_i).sum(dim=-2)    # What does memory say about k_i?\n",
    "3. DELTA:   v_new = (v_i - recalled) * β_i      # What's genuinely new?\n",
    "4. WRITE:   S = S + k_i ⊗ v_new                 # Store the new info (rank-1 update)\n",
    "5. READ:    o_i = q_i @ S                        # Query the memory\n",
    "```\n",
    "\n",
    "The **delta correction** (step 2-3) is what makes this different from a simple linear RNN:\n",
    "- It checks what the memory *already knows* about key `k_i`\n",
    "- Only writes the *difference* (delta) between the new value and the recalled value\n",
    "- This prevents redundant writing and improves memory utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Rule Recurrence — step by step for ONE head\n",
    "head = 0  # Let's trace head 0\n",
    "\n",
    "# Extract single head tensors\n",
    "q_h = q[0, head].float()    # [T, d_k]\n",
    "k_h = k[0, head].float()    # [T, d_k]\n",
    "v_h = v[0, head].float()    # [T, d_v]\n",
    "beta_h = beta[0, head].float()  # [T]\n",
    "g_h = gk[0, head].float()      # [T]\n",
    "\n",
    "# Initialize state matrix\n",
    "S = torch.zeros(d_k, d_v)  # The memory!\n",
    "o_h = torch.zeros(T, d_v)  # Output accumulator\n",
    "\n",
    "print(f\"State matrix S shape: [{d_k}, {d_v}] = {d_k*d_v} parameters\")\n",
    "print(f\"Processing {T} tokens sequentially...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(T):\n",
    "    _k = k_h[i]        # [d_k] — current key\n",
    "    _q = q_h[i]        # [d_k] — current query  \n",
    "    _v = v_h[i].clone() # [d_v] — current value\n",
    "    _beta = beta_h[i]   # scalar — write gate\n",
    "    _g = g_h[i]         # scalar — decay gate (log-space)\n",
    "    \n",
    "    print(f\"\\n--- Token {i} ---\")\n",
    "    print(f\"  β (write gate) = {_beta.item():.4f}\")\n",
    "    print(f\"  g (decay, log)  = {_g.item():.4f}, exp(g) = {_g.exp().item():.4f}\")\n",
    "    \n",
    "    # 1. DECAY: Forget old information\n",
    "    S_before_decay = S.clone()\n",
    "    decay = _g.exp()  # scalar in (0, 1)\n",
    "    S = S * decay\n",
    "    print(f\"  1. DECAY: S *= {decay.item():.4f}\")\n",
    "    print(f\"     S Frobenius norm: {S_before_decay.norm():.4f} → {S.norm():.4f}\")\n",
    "    \n",
    "    # 2. RECALL: What does memory already know about this key?\n",
    "    recalled = (S * _k[..., None]).sum(dim=-2)  # [d_v]\n",
    "    print(f\"  2. RECALL: || recalled || = {recalled.norm():.4f}\")\n",
    "    \n",
    "    # 3. DELTA: Only write what's genuinely new\n",
    "    delta = _v - recalled\n",
    "    v_new = delta * _beta\n",
    "    print(f\"  3. DELTA: || v_original || = {_v.norm():.4f}\")\n",
    "    print(f\"            || v - recalled || = {delta.norm():.4f}\")\n",
    "    print(f\"            || v_new (delta * β) || = {v_new.norm():.4f}\")\n",
    "    \n",
    "    # 4. WRITE: Rank-1 update to state\n",
    "    S_before_write = S.clone()\n",
    "    S = S + _k.unsqueeze(-1) * v_new.unsqueeze(-2)  # outer product\n",
    "    print(f\"  4. WRITE: S += k ⊗ v_new (rank-1 update)\")\n",
    "    print(f\"     S Frobenius norm: {S_before_write.norm():.4f} → {S.norm():.4f}\")\n",
    "    \n",
    "    # 5. READ: Query the memory\n",
    "    o_h[i] = torch.einsum('d,dm->m', _q, S)  # [d_v]\n",
    "    print(f\"  5. READ: || output || = {o_h[i].norm():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nFinal state S norm: {S.norm():.4f}\")\n",
    "print(f\"Output shape: {o_h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Full Recurrence (All Heads)\n",
    "\n",
    "Now let's run the complete `recurrent_gated_delta_rule_ref` on all heads simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_gated_delta_rule_ref(q, k, v, beta, g):\n",
    "    \"\"\"Reference recurrence from NVlabs/GatedDeltaNet\"\"\"\n",
    "    q, k, v, beta, g = map(lambda x: x.to(torch.float32), [q, k, v, beta, g])\n",
    "    b, h, l, d_k = q.shape\n",
    "    d_v = v.shape[-1]\n",
    "    o = torch.zeros_like(v)\n",
    "    S = torch.zeros(b, h, d_k, d_v).to(v)\n",
    "    \n",
    "    for i in range(l):\n",
    "        _k = k[:, :, i]\n",
    "        _q = q[:, :, i]\n",
    "        _v = v[:, :, i].clone()\n",
    "        S = S.clone() * g[:, :, i].exp()[..., None, None]\n",
    "        beta_i = beta[:, :, i]\n",
    "        _v = _v - (S.clone() * _k[..., None]).sum(-2)\n",
    "        _v = _v * beta_i[..., None]\n",
    "        S = S.clone() + _k.unsqueeze(-1) * _v.unsqueeze(-2)\n",
    "        o[:, :, i] = torch.einsum('bhd,bhdm->bhm', _q, S)\n",
    "    \n",
    "    return o\n",
    "\n",
    "# Run on all heads\n",
    "output = recurrent_gated_delta_rule_ref(q, k, v, beta, gk.detach())\n",
    "print(f\"Full recurrence output shape: {output.shape} = [B, H, T, d_v]\")\n",
    "\n",
    "# Verify head 0 matches our step-by-step\n",
    "diff = (output[0, head] - o_h).abs().max()\n",
    "print(f\"\\nMax difference between step-by-step and batched (head {head}): {diff:.6e}\")\n",
    "print(f\"Match: {'YES ✓' if diff < 1e-5 else 'NO ✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Output Normalization with Swish Gate\n",
    "\n",
    "The output goes through `FusedRMSNormSwishGate`:\n",
    "\n",
    "```\n",
    "output = gate * SiLU(RMSNorm(recurrence_output))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `RMSNorm(x) = x / sqrt(mean(x²) + ε) * weight`\n",
    "- `SiLU(x) = x * sigmoid(x)` (smooth activation)\n",
    "- `gate = g_proj(x)` (learned multiplicative gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose back: [B, H, T, d_v] -> [B, T, H, d_v]\n",
    "o = output.transpose(1, 2)\n",
    "\n",
    "# Compute gate from original input\n",
    "g = g_proj(x).view(B, T, H, d_v)\n",
    "\n",
    "print(f\"Recurrence output o: {o.shape}\")\n",
    "print(f\"Gate g: {g.shape}\")\n",
    "\n",
    "# RMSNorm\n",
    "norm_weight = nn.Parameter(torch.ones(d_v))\n",
    "eps = 1e-6\n",
    "\n",
    "# Process per-token, per-head\n",
    "o_flat = o.reshape(B * T * H, d_v)\n",
    "g_flat = g.reshape(B * T * H, d_v)\n",
    "\n",
    "# RMSNorm\n",
    "variance = o_flat.pow(2).mean(-1, keepdim=True)\n",
    "o_normed = o_flat * torch.rsqrt(variance + eps) * norm_weight\n",
    "\n",
    "# Swish gate: gate * SiLU(normalized_output)\n",
    "o_gated = g_flat * F.silu(o_normed)\n",
    "\n",
    "print(f\"\\nAfter RMSNorm: mean variance = {variance.mean():.4f}\")\n",
    "print(f\"After SiLU + gate: shape = {o_gated.shape}\")\n",
    "\n",
    "# Reshape back\n",
    "o_final = o_gated.view(B, T, value_dim)\n",
    "print(f\"\\nReshaped output: {o_final.shape} = [B, T, value_dim={value_dim}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Output Projection\n",
    "\n",
    "Finally, project back from `value_dim` to `hidden_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_proj = nn.Linear(value_dim, D, bias=False)\n",
    "final_output = o_proj(o_final)\n",
    "\n",
    "print(f\"Output projection: [{value_dim}] → [{D}]\")\n",
    "print(f\"Final output shape: {final_output.shape} = [B, T, D]\")\n",
    "print(f\"\\nThis matches the input shape — GDN is a drop-in replacement for attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete GDN Forward Pass\n",
    "\n",
    "| Step | Operation | Shape Transform | Purpose |\n",
    "|------|-----------|----------------|----------|\n",
    "| 1 | q/k/v_proj | [B,T,D] → [B,T,key_dim/value_dim] | Project to QKV |\n",
    "| 2 | Short Conv + SiLU | Same shape | Local context mixing |\n",
    "| 3 | Reshape | → [B,T,H,d_k/d_v] | Multi-head format |\n",
    "| 4 | RoPE | Same shape | Positional encoding |\n",
    "| 5 | L2 Norm | Same shape | Stabilize recurrence |\n",
    "| 6 | Gates (β, g) | [B,T,D] → [B,H,T] | Control write/decay |\n",
    "| 7 | Transpose | → [B,H,T,d] | Head-first for recurrence |\n",
    "| 8 | Scale Q | Same shape | Normalize attention |\n",
    "| 9 | Delta Rule | [B,H,T,d_k]×State → [B,H,T,d_v] | **Core recurrence** |\n",
    "| 10 | RMSNorm+SwishGate | → [B,T,value_dim] | Output normalization |\n",
    "| 11 | o_proj | → [B,T,D] | Back to model dim |\n",
    "\n",
    "### Complexity\n",
    "- **Time**: O(T × d_k × d_v) per head — **linear in sequence length**\n",
    "- **Space**: O(d_k × d_v) per head for the state matrix — **constant in sequence length**\n",
    "- **vs Standard Attention**: O(T²×d) time and O(T²) space\n",
    "\n",
    "### Why It Works\n",
    "1. **Delta correction** prevents redundant writes → better memory utilization\n",
    "2. **Gated decay** allows selective forgetting → handles long-range dependencies\n",
    "3. **L2 normalization** bounds state growth → stable training\n",
    "4. **Asymmetric key/value dims** (expand_k=0.75, expand_v=1.5) → compact keys, rich values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the state matrix evolution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Re-run recurrence tracking state norms\n",
    "q_h = q[0, 0].float()\n",
    "k_h = k[0, 0].float()\n",
    "v_h = v[0, 0].float()\n",
    "beta_h = beta[0, 0].float()\n",
    "g_h = gk[0, 0].detach().float()\n",
    "\n",
    "S = torch.zeros(d_k, d_v)\n",
    "state_norms = []\n",
    "decay_factors = []\n",
    "delta_norms = []\n",
    "\n",
    "for i in range(T):\n",
    "    _k, _q, _v = k_h[i], q_h[i], v_h[i].clone()\n",
    "    _beta, _g = beta_h[i], g_h[i]\n",
    "    \n",
    "    decay = _g.exp()\n",
    "    S = S * decay\n",
    "    recalled = (S * _k[..., None]).sum(dim=-2)\n",
    "    delta = _v - recalled\n",
    "    v_new = delta * _beta\n",
    "    S = S + _k.unsqueeze(-1) * v_new.unsqueeze(-2)\n",
    "    \n",
    "    state_norms.append(S.norm().item())\n",
    "    decay_factors.append(decay.item())\n",
    "    delta_norms.append(delta.norm().item())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].bar(range(T), state_norms, color='steelblue')\n",
    "axes[0].set_xlabel('Token Position')\n",
    "axes[0].set_ylabel('Frobenius Norm')\n",
    "axes[0].set_title('State Matrix ||S|| Over Time')\n",
    "\n",
    "axes[1].bar(range(T), decay_factors, color='coral')\n",
    "axes[1].set_xlabel('Token Position')\n",
    "axes[1].set_ylabel('exp(g)')\n",
    "axes[1].set_title('Decay Factor (Retention)')\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "\n",
    "axes[2].bar(range(T), delta_norms, color='mediumseagreen')\n",
    "axes[2].set_xlabel('Token Position')\n",
    "axes[2].set_ylabel('||v - recalled||')\n",
    "axes[2].set_title('Delta Norm (New Information)')\n",
    "\n",
    "plt.suptitle('GDN State Evolution — Head 0', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}