{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: Gated DeltaNet vs Gated Sparse Attention\\n\n",
    "Comparison of throughput and memory usage.\\n\n",
    "**Reference Implementations:**\\n\n",
    "- GDN: [NVlabs/GatedDeltaNet](https://github.com/NVlabs/GatedDeltaNet)\\n\n",
    "- GSA: [alfredcs/Gated-Sparse-Attention](https://github.com/alfredcs/Gated-Sparse-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports & Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"Benchmark\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Mock HAS_TRITON to False for portability (or check if installed)\n",
    "try:\n",
    "    import triton\n",
    "    HAS_TRITON = True\n",
    "except ImportError:\n",
    "    HAS_TRITON = False\n",
    "    print(\"Triton not found. Using PyTorch fallbacks.\")\n",
    "\n",
    "# Disable gradients for benchmarking\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Kernel Fallbacks\n",
    "# Aligned with:\n",
    "#   GSA: https://github.com/alfredcs/Gated-Sparse-Attention/blob/main/gsa/kernels/\n",
    "#   GDN: https://github.com/NVlabs/GatedDeltaNet/blob/main/lit_gpt/gated_delta_rule_ops/\n",
    "# ============================================================================\n",
    "\n",
    "def pytorch_gated_indexer(\n",
    "    q: torch.Tensor, k: torch.Tensor, w: torch.Tensor, b: torch.Tensor,\n",
    "    scale: float = 1.0, causal: bool = True, q_offset: int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    PyTorch fallback for GatedLightningIndexer scoring.\n",
    "    Matches: gsa/kernels/triton_indexer.py -> triton_gated_indexer()\n",
    "    \n",
    "    Formula per ref: score[q,k] = sum_h( sigmoid(w[q,h]) * sigmoid(dot(q_I[q,h], k_I[k]) * scale + b[h]) )\n",
    "    \n",
    "    Args:\n",
    "        q: [batch, seq_q, n_heads, d_idx] - indexer queries\n",
    "        k: [batch, seq_kv, d_idx] - indexer keys (shared across heads)\n",
    "        w: [batch, seq_q, n_heads] - query-dependent importance weights\n",
    "        b: [n_heads] - learnable bias per indexer head\n",
    "        scale: 1/sqrt(d_idx)\n",
    "    Returns:\n",
    "        scores: [batch, seq_q, seq_kv]\n",
    "    \"\"\"\n",
    "    batch_size, seq_q, n_heads, d_idx = q.shape\n",
    "    seq_kv = k.shape[1]\n",
    "\n",
    "    q = q.float()\n",
    "    k = k.float()\n",
    "    \n",
    "    # QK dot product per indexer head: [batch, n_heads, seq_q, seq_kv]\n",
    "    raw_scores = torch.einsum('bqhd,bkd->bhqk', q, k) * scale\n",
    "\n",
    "    # Sigmoid gating with learnable bias (ref: sigmoid(dot + b))\n",
    "    bias_expanded = b.float().view(1, -1, 1, 1)\n",
    "    gated_scores = torch.sigmoid(raw_scores + bias_expanded)\n",
    "\n",
    "    # Query-dependent importance: sigmoid(w) (ref: weight_proj -> sigmoid)\n",
    "    w_sigmoid = torch.sigmoid(w.float()).permute(0, 2, 1).unsqueeze(-1)  # [batch, n_heads, seq_q, 1]\n",
    "\n",
    "    # Weighted sum across indexer heads (ref: weighted_scores.sum(dim=1))\n",
    "    weighted_scores = gated_scores * w_sigmoid\n",
    "    final_scores = weighted_scores.sum(dim=1)  # [batch, seq_q, seq_kv]\n",
    "\n",
    "    # Causal mask (ref: torch.triu diagonal=1 mask)\n",
    "    if causal:\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(seq_q, seq_kv, device=q.device, dtype=torch.bool),\n",
    "            diagonal=1 + q_offset\n",
    "        )\n",
    "        if q_offset > 0:\n",
    "            query_positions = q_offset + torch.arange(seq_q, device=q.device)\n",
    "            key_positions = torch.arange(seq_kv, device=q.device)\n",
    "            causal_mask = key_positions.unsqueeze(0) > query_positions.unsqueeze(1)\n",
    "        final_scores = final_scores.masked_fill(causal_mask.unsqueeze(0), float('-inf'))\n",
    "\n",
    "    return final_scores\n",
    "\n",
    "def pytorch_rmsnorm(x: torch.Tensor, weight: torch.Tensor, eps: float = 1e-6, residual: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    \"\"\"RMSNorm matching lit_gpt/rmsnorm.py\"\"\"\n",
    "    if residual is not None:\n",
    "        x = x + residual\n",
    "    variance = x.pow(2).mean(-1, keepdim=True)\n",
    "    x_normed = x * torch.rsqrt(variance + eps)\n",
    "    return x_normed * weight\n",
    "\n",
    "def pytorch_sparse_attention(q, k, v, indices, mask, scale):\n",
    "    \"\"\"\n",
    "    PyTorch fallback for sparse attention.\n",
    "    Matches: gsa/kernels/triton_sparse_attn.py -> _pytorch_sparse_attention()\n",
    "    \n",
    "    Reference uses gather along seq dim with 5D expand:\n",
    "        indices_expanded = indices.unsqueeze(-1).unsqueeze(-1).expand(B, T, k_sel, H, D)\n",
    "        x_expanded = x.unsqueeze(1).expand(B, T, seq_kv, H, D)\n",
    "        gathered = torch.gather(x_expanded, 2, indices_expanded)\n",
    "    \n",
    "    Args:\n",
    "        q: [B, T, H, D] query tensor\n",
    "        k: [B, T_kv, H, D] key tensor\n",
    "        v: [B, T_kv, H, D] value tensor\n",
    "        indices: [B, T, k_selected] indices for token selection (shared across heads)\n",
    "        mask: [B, T, k_selected] boolean mask (True=valid)\n",
    "        scale: attention scale factor\n",
    "    Returns:\n",
    "        output: [B, T, H, D]\n",
    "    \"\"\"\n",
    "    B, T, H, D = q.shape\n",
    "    T_kv = k.shape[1]\n",
    "    k_selected = indices.shape[-1]\n",
    "    \n",
    "    # Clamp indices to valid range\n",
    "    idx = indices.clamp(0, T_kv - 1).long()  # [B, T, k_selected]\n",
    "    \n",
    "    # Gather K and V along sequence dimension (matching ref _gather_along_seq)\n",
    "    # idx: [B, T, k_sel] -> [B, T, k_sel, H, D]\n",
    "    idx_exp = idx.unsqueeze(-1).unsqueeze(-1).expand(B, T, k_selected, H, D)\n",
    "    \n",
    "    # k: [B, T_kv, H, D] -> [B, T, T_kv, H, D] (expand for each query position)\n",
    "    k_expanded = k.unsqueeze(1).expand(B, T, T_kv, H, D)\n",
    "    v_expanded = v.unsqueeze(1).expand(B, T, T_kv, H, D)\n",
    "    \n",
    "    # Gather: [B, T, k_selected, H, D]\n",
    "    k_gathered = torch.gather(k_expanded, 2, idx_exp)\n",
    "    v_gathered = torch.gather(v_expanded, 2, idx_exp)\n",
    "    \n",
    "    # Permute for attention: [B, T, H, k_selected, D]\n",
    "    k_gathered = k_gathered.permute(0, 1, 3, 2, 4)\n",
    "    v_gathered = v_gathered.permute(0, 1, 3, 2, 4)\n",
    "    \n",
    "    # Attention scores: [B, T, H, k_selected]\n",
    "    scores = torch.einsum('bqhd,bqhkd->bqhk', q, k_gathered) * scale\n",
    "    \n",
    "    # Apply mask: [B, T, k_selected] -> [B, T, 1, k_selected]\n",
    "    mask_expanded = mask.unsqueeze(2)\n",
    "    scores = scores.masked_fill(~mask_expanded, float('-inf'))\n",
    "    \n",
    "    # Softmax over k_selected dimension\n",
    "    attn_weights = torch.softmax(scores, dim=-1)\n",
    "    attn_weights = attn_weights.masked_fill(~mask_expanded, 0.0)\n",
    "    attn_weights = attn_weights.nan_to_num(0.0)\n",
    "    \n",
    "    # Weighted sum: [B, T, H, D]\n",
    "    output = torch.einsum('bqhk,bqhkd->bqhd', attn_weights, v_gathered)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def recurrent_gated_delta_rule_ref(q, k, v, beta, g):\n",
    "    \"\"\"\n",
    "    Reference recurrent delta rule from NVlabs/GatedDeltaNet.\n",
    "    Source: lit_gpt/gated_delta_rule_ops/chunk.py -> recurrent_gated_delta_rule_ref()\n",
    "    \n",
    "    This is the pure recurrence (no chunking) used for correctness verification.\n",
    "    \n",
    "    Args:\n",
    "        q: [B, H, T, d_k] queries (pre-scaled by d_k^-0.5)\n",
    "        k: [B, H, T, d_k] keys\n",
    "        v: [B, H, T, d_v] values\n",
    "        beta: [B, H, T] write gate (sigmoid output)\n",
    "        g: [B, H, T] decay gate (log-space, will be exp'd)\n",
    "    Returns:\n",
    "        o: [B, H, T, d_v] output\n",
    "    \"\"\"\n",
    "    q, k, v, beta, g = map(lambda x: x.to(torch.float32), [q, k, v, beta, g])\n",
    "    b, h, l, d_k = q.shape\n",
    "    d_v = v.shape[-1]\n",
    "    o = torch.zeros_like(v)\n",
    "    S = torch.zeros(b, h, d_k, d_v).to(v)\n",
    "    \n",
    "    for i in range(l):\n",
    "        _k = k[:, :, i]\n",
    "        _q = q[:, :, i]\n",
    "        _v = v[:, :, i].clone()\n",
    "        # Decay state\n",
    "        S = S.clone() * g[:, :, i].exp()[..., None, None]\n",
    "        beta_i = beta[:, :, i]\n",
    "        # Delta rule: subtract current memory readout from value\n",
    "        _v = _v - (S.clone() * _k[..., None]).sum(-2)\n",
    "        _v = _v * beta_i[..., None]\n",
    "        # Update state: rank-1 outer product\n",
    "        S = S.clone() + _k.unsqueeze(-1) * _v.unsqueeze(-2)\n",
    "        # Output: query the state\n",
    "        o[:, :, i] = torch.einsum('bhd,bhdm->bhm', _q, S)\n",
    "    \n",
    "    return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Core Components\n",
    "# Aligned with:\n",
    "#   GDN: lit_gpt/rmsnorm.py, lit_gpt/rotary.py, lit_gpt/gated_delta_net.py\n",
    "#   GSA: gsa/attention/rope.py\n",
    "# ============================================================================\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Matches lit_gpt/rmsnorm.py\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return pytorch_rmsnorm(x, self.weight, self.eps)\n",
    "\n",
    "class FusedRMSNormSwishGate(nn.Module):\n",
    "    \"\"\"Matches lit_gpt/gated_delta_net.py -> g_norm_swish_gate\"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.norm = RMSNorm(dim, eps)\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        x_norm = self.norm(x)\n",
    "        return g * F.silu(x_norm)\n",
    "\n",
    "def _rotate_half(x):\n",
    "    \"\"\"Matches gsa/attention/rope.py -> _rotate_half()\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    \"\"\"\n",
    "    Matches gsa/attention/rope.py -> apply_rotary_pos_emb()\n",
    "    Standard RoPE: x * cos + rotate_half(x) * sin\n",
    "    \"\"\"\n",
    "    q_embed = (q * cos) + (_rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (_rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Matches gsa/attention/rope.py -> RotaryEmbedding\n",
    "    Standard RoPE with optional NTK scaling.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, max_position_embeddings: int = 8192, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        \n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        # Pre-compute cache\n",
    "        self._set_cos_sin_cache(max_position_embeddings)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len):\n",
    "        t = torch.arange(seq_len, dtype=torch.float32)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype=None):\n",
    "        if seq_len > self.cos_cached.shape[0]:\n",
    "            self._set_cos_sin_cache(seq_len)\n",
    "            self.cos_cached = self.cos_cached.to(device)\n",
    "            self.sin_cached = self.sin_cached.to(device)\n",
    "        cos = self.cos_cached[:seq_len]\n",
    "        sin = self.sin_cached[:seq_len]\n",
    "        if dtype is not None:\n",
    "            cos = cos.to(dtype)\n",
    "            sin = sin.to(dtype)\n",
    "        return cos, sin\n",
    "\n",
    "def l2_norm_fn(x):\n",
    "    \"\"\"L2 normalization matching lit_gpt/gated_delta_net.py -> l2_norm_fn\"\"\"\n",
    "    return F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "class ShortConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Matches lit_gpt/gated_delta_net.py -> ShortConvolution\n",
    "    Depthwise 1D convolution with optional activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, conv_size=4, activation='silu'):\n",
    "        super().__init__()\n",
    "        self.conv_size = conv_size\n",
    "        self.conv = nn.Conv1d(dim, dim, kernel_size=conv_size, padding=conv_size - 1, groups=dim)\n",
    "        self.activation = nn.SiLU() if activation == 'silu' else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask=None, cache=None):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :-(self.conv_size - 1)]\n",
    "        x = x.transpose(1, 2)\n",
    "        return self.activation(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Gated DeltaNet\n",
    "# Aligned with: https://github.com/NVlabs/GatedDeltaNet/blob/main/lit_gpt/gated_delta_net.py\n",
    "#\n",
    "# Key design choices from reference:\n",
    "#   - expand_k=0.75, expand_v=1.5 (key_dim != value_dim)\n",
    "#   - Mamba-style gating: gk = -A.exp() * softplus(gk + dt_bias)\n",
    "#   - L2 normalization on Q, K\n",
    "#   - q scaled by d_k^(-0.5) inside recurrence (not separate)\n",
    "#   - Delta rule: S = S*g.exp() + k*(v*beta - (S*k).sum(-2)*beta) [rank-1 update]\n",
    "#   - FusedRMSNormSwishGate on output with gate projection\n",
    "# ============================================================================\n",
    "\n",
    "class GatedDeltaNet(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, head_dim=None, \n",
    "                 expand_k=0.75, expand_v=1.5,\n",
    "                 max_seq_len=262144, conv_size=4, qk_norm='l2',\n",
    "                 use_mamba_gate=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.qk_norm = qk_norm\n",
    "        \n",
    "        # Ref: key_dim = int(hidden_size * expand_k), value_dim = int(hidden_size * expand_v)\n",
    "        self.key_dim = int(hidden_size * expand_k)\n",
    "        self.value_dim = int(hidden_size * expand_v)\n",
    "        self.head_qk_dim = self.key_dim // num_heads\n",
    "        self.head_v_dim = self.value_dim // num_heads\n",
    "\n",
    "        # Projections (ref: q_proj, k_proj -> key_dim; v_proj -> value_dim_per_group)\n",
    "        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(hidden_size, self.key_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(hidden_size, self.value_dim, bias=False)\n",
    "        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False)\n",
    "\n",
    "        # Gate projections (ref: b_proj with bias=True, gk_proj with bias=False when mamba_gate)\n",
    "        self.b_proj = nn.Linear(hidden_size, num_heads, bias=True)\n",
    "        self.gk_proj = nn.Linear(hidden_size, num_heads, bias=not use_mamba_gate)\n",
    "\n",
    "        # Short convolutions (ref: q,k use key_dim; v uses value_dim)\n",
    "        self.q_conv1d = ShortConvolution(self.key_dim, conv_size=conv_size, activation='silu')\n",
    "        self.k_conv1d = ShortConvolution(self.key_dim, conv_size=conv_size, activation='silu')\n",
    "        self.v_conv1d = ShortConvolution(self.value_dim, conv_size=conv_size, activation='silu')\n",
    "\n",
    "        # Mamba-style gate parameters (ref: A_log, D, dt_bias initialization)\n",
    "        self.use_mamba_gate = use_mamba_gate\n",
    "        if use_mamba_gate:\n",
    "            A = torch.empty(num_heads, dtype=torch.float32).uniform_(0, 16)\n",
    "            self.A_log = nn.Parameter(torch.log(A))\n",
    "            self.D = nn.Parameter(torch.ones(num_heads))\n",
    "            # dt_bias initialization matching ref\n",
    "            dt_min, dt_max = 0.001, 0.1\n",
    "            dt = torch.exp(\n",
    "                torch.rand(num_heads) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)\n",
    "            )\n",
    "            dt = torch.clamp(dt, min=1e-4)\n",
    "            inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "            self.dt_bias = nn.Parameter(inv_dt)\n",
    "\n",
    "        # RoPE\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_qk_dim, max_seq_len)\n",
    "\n",
    "        # Output norm (ref: FusedRMSNormSwishGate on head_v_dim)\n",
    "        self.g_norm_swish_gate = FusedRMSNormSwishGate(self.head_v_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Step 1: Projections (ref: q_proj, k_proj, v_proj)\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # Step 2: Short convolutions (ref: q_conv1d, k_conv1d, v_conv1d with SiLU)\n",
    "        q = self.q_conv1d(q)\n",
    "        k = self.k_conv1d(k)\n",
    "        v = self.v_conv1d(v)\n",
    "\n",
    "        # Step 3: Reshape to heads\n",
    "        q = q.view(B, T, self.num_heads, self.head_qk_dim)  # [B, T, H, d_k]\n",
    "        k = k.view(B, T, self.num_heads, self.head_qk_dim)  # [B, T, H, d_k]\n",
    "        v = v.view(B, T, self.num_heads, self.head_v_dim)   # [B, T, H, d_v]\n",
    "\n",
    "        # Step 4: RoPE on Q, K\n",
    "        cos, sin = self.rotary_emb(T, device, x.dtype)\n",
    "        cos = cos.unsqueeze(0).unsqueeze(2)  # [1, T, 1, d_k]\n",
    "        sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        # Step 5: L2 normalize Q, K (ref: qk_norm == 'l2')\n",
    "        q = l2_norm_fn(q)\n",
    "        k = l2_norm_fn(k)\n",
    "\n",
    "        # Step 6: Compute gates\n",
    "        # beta (write gate): sigmoid (ref: b_proj -> sigmoid -> transpose)\n",
    "        beta = self.b_proj(x).float().sigmoid()  # [B, T, H]\n",
    "        beta = beta.transpose(1, 2)  # [B, H, T]\n",
    "\n",
    "        # gk (decay gate): -A.exp() * softplus(gk + dt_bias) (ref: Mamba-style)\n",
    "        gk = self.gk_proj(x).float()  # [B, T, H]\n",
    "        if self.use_mamba_gate:\n",
    "            gk = -self.A_log.float().exp() * F.softplus(gk + self.dt_bias)\n",
    "        gk = gk.transpose(1, 2)  # [B, H, T]\n",
    "\n",
    "        # Step 7: Rearrange for recurrence [B, T, H, d] -> [B, H, T, d]\n",
    "        q = q.transpose(1, 2)  # [B, H, T, d_k]\n",
    "        k = k.transpose(1, 2)  # [B, H, T, d_k]\n",
    "        v = v.transpose(1, 2)  # [B, H, T, d_v]\n",
    "\n",
    "        # Step 8: Scale queries (ref: q = q * (d_k ** -0.5))\n",
    "        q = q * (self.head_qk_dim ** -0.5)\n",
    "\n",
    "        # Step 9: Delta rule recurrence (ref: recurrent_gated_delta_rule_ref)\n",
    "        o = recurrent_gated_delta_rule_ref(q, k, v, beta, gk)\n",
    "\n",
    "        # Step 10: Transpose back and apply output norm with gate\n",
    "        # ref: o = rearrange(o, 'b h l d -> b l h d')\n",
    "        o = o.transpose(1, 2)  # [B, T, H, d_v]\n",
    "        g = self.g_proj(x).view(B, T, self.num_heads, self.head_v_dim)\n",
    "\n",
    "        # ref: g_norm_swish_gate(o, g) then reshape\n",
    "        o_flat = o.reshape(B * T * self.num_heads, self.head_v_dim)\n",
    "        g_flat = g.reshape(B * T * self.num_heads, self.head_v_dim)\n",
    "        o_normed = self.g_norm_swish_gate(o_flat, g_flat)\n",
    "        o = o_normed.view(B, T, self.value_dim)\n",
    "\n",
    "        # Step 11: Output projection\n",
    "        return self.o_proj(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/NVlabs/GatedDeltaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Benchmarking Infrastructure\n",
    "# ============================================================================\n",
    "\n",
    "def benchmark_run(model_cls, name, configs, device, verbose=True):\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Benchmarking {name}...\")\n",
    "    print(f\"{'B':<4} {'T':<6} {'D':<6} | {'Time (ms)':<10} | {'Tokens/s':<10} | {'Mem (MB)':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for (B, T, D) in configs:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        num_heads = 16\n",
    "        \n",
    "        # Instantiate model with reference-aligned constructors\n",
    "        try:\n",
    "            if name == \"GatedDeltaNet\":\n",
    "                # GDN uses expand_k/expand_v, not explicit head_dim\n",
    "                model = model_cls(D, num_heads).to(device).to(torch.bfloat16)\n",
    "            else:\n",
    "                # GSA with reference defaults: d_indexer=64, k_base=2048, k_min=256, k_max=min(4096,T)\n",
    "                model = model_cls(D, num_heads, k_max=min(4096, T)).to(device).to(torch.bfloat16)\n",
    "            \n",
    "            model.eval()\n",
    "            x = torch.randn(B, T, D, device=device, dtype=torch.bfloat16)\n",
    "            \n",
    "            # Warmup (gradients already globally disabled via torch.set_grad_enabled(False))\n",
    "            for _ in range(5):\n",
    "                _ = model(x)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            \n",
    "            start_event.record()\n",
    "            for _ in range(10):\n",
    "                _ = model(x)\n",
    "            end_event.record()\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            elapsed_ms = start_event.elapsed_time(end_event) / 10.0\n",
    "            tokens_per_sec = (B * T) / (elapsed_ms / 1000.0)\n",
    "            mem_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "            \n",
    "            print(f\"{B:<4} {T:<6} {D:<6} | {elapsed_ms:<10.2f} | {tokens_per_sec:<10.0f} | {mem_mb:<10.0f}\")\n",
    "            \n",
    "            results.append({\n",
    "                \"B\": B, \"T\": T, \"D\": D,\n",
    "                \"time_ms\": elapsed_ms,\n",
    "                \"throughput\": tokens_per_sec,\n",
    "                \"memory_mb\": mem_mb\n",
    "            })\n",
    "            \n",
    "            del model, x\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"{B:<4} {T:<6} {D:<6} | {'OOM':<10} | {'-':<10} | {'-':<10}\")\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                print(f\"{B:<4} {T:<6} {D:<6} | {'ERROR':<10} | {'-':<10} | {str(e)[:30]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_results(results_delta, results_gsa):\n",
    "    ts_delta = sorted(list(set(r[\"T\"] for r in results_delta)))\n",
    "    ts_gsa = sorted(list(set(r[\"T\"] for r in results_gsa)))\n",
    "    ts = sorted(list(set(ts_delta + ts_gsa)))\n",
    "    \n",
    "    # Throughput\n",
    "    tp_delta = [next((r[\"throughput\"] for r in results_delta if r[\"T\"] == t), 0) for t in ts]\n",
    "    tp_gsa = [next((r[\"throughput\"] for r in results_gsa if r[\"T\"] == t), 0) for t in ts]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].plot(ts, tp_delta, marker='o', label='GatedDeltaNet (NVlabs)')\n",
    "    axes[0].plot(ts, tp_gsa, marker='x', label='GSA (alfredcs)')\n",
    "    axes[0].set_xlabel(\"Sequence Length\")\n",
    "    axes[0].set_ylabel(\"Tokens / Sec\")\n",
    "    axes[0].set_title(\"Throughput vs Sequence Length\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Memory\n",
    "    mem_delta = [next((r[\"memory_mb\"] for r in results_delta if r[\"T\"] == t), 0) for t in ts]\n",
    "    mem_gsa = [next((r[\"memory_mb\"] for r in results_gsa if r[\"T\"] == t), 0) for t in ts]\n",
    "    \n",
    "    axes[1].plot(ts, mem_delta, marker='o', label='GatedDeltaNet (NVlabs)')\n",
    "    axes[1].plot(ts, mem_gsa, marker='x', label='GSA (alfredcs)')\n",
    "    axes[1].set_xlabel(\"Sequence Length\")\n",
    "    axes[1].set_ylabel(\"Peak Memory (MB)\")\n",
    "    axes[1].set_title(\"Memory vs Sequence Length\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run Benchmarks\n",
    "# Define configurations: [(B, T, D)]\n",
    "configs = [\n",
    "    (1, 1024, 2048),\n",
    "    (1, 2048, 2048),\n",
    "    (1, 4096, 2048),\n",
    "    (1, 8192, 2048),\n",
    "    # Uncomment for larger runs if GPU permits (PyTorch fallback is slow!)\n",
    "    # (1, 16384, 2048),\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    results_delta = benchmark_run(GatedDeltaNet, \"GatedDeltaNet\", configs, device)\n",
    "    print(\"\\n\")\n",
    "    results_gsa = benchmark_run(GatedSparseAttention, \"GatedSparseAttention\", configs, device)\n",
    "    \n",
    "    plot_results(results_delta, results_gsa)\n",
    "else:\n",
    "    print(\"Skipping benchmark execution (requires CUDA for timing/memory).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
