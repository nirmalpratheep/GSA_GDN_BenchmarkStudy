{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: Gated DeltaNet vs Gated Sparse Attention\\n\n",
    "Comparison of throughput and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports & Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"Benchmark\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Mock HAS_TRITON to False for portability (or check if installed)\n",
    "try:\n",
    "    import triton\n",
    "    HAS_TRITON = True\n",
    "except ImportError:\n",
    "    HAS_TRITON = False\n",
    "    print(\"Triton not found. Using PyTorch fallbacks.\")\n",
    "\n",
    "# Disable gradients for benchmarking\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Kernel Fallbacks (Extracted from src/kernels/)\n# ============================================================================\n\ndef pytorch_gated_indexer(\n    q: torch.Tensor, k: torch.Tensor, w: torch.Tensor, b: torch.Tensor,\n    scale: float = 1.0, causal: bool = True, q_offset: int = 0\n) -> torch.Tensor:\n    \"\"\"PyTorch fallback for gated indexer computation.\"\"\"\n    batch_size, seq_q, n_heads, d_idx = q.shape\n    seq_kv = k.shape[1]\n\n    # Compute QK scores per head: [batch, n_heads, seq_q, seq_kv]\n    # Use float32 for stability\n    q = q.float()\n    k = k.float()\n    \n    raw_scores = torch.einsum('bqhd,bkd->bhqk', q, k) * scale\n\n    # Add bias: [n_heads, 1, 1]\n    bias_expanded = b.float().view(1, -1, 1, 1)\n\n    # Apply sigmoid activation\n    gated_scores = torch.sigmoid(raw_scores + bias_expanded)\n\n    # Weight by query-dependent importance\n    w_sigmoid = torch.sigmoid(w.float()).permute(0, 2, 1).unsqueeze(-1)\n\n    # Weighted sum across heads\n    weighted_scores = gated_scores * w_sigmoid\n    final_scores = weighted_scores.sum(dim=1)  # [batch, seq_q, seq_kv]\n\n    # Apply causal mask\n    if causal:\n        query_positions = q_offset + torch.arange(seq_q, device=q.device)\n        key_positions = torch.arange(seq_kv, device=q.device)\n        causal_invalid = key_positions.unsqueeze(0) > query_positions.unsqueeze(1)\n        final_scores = final_scores.masked_fill(causal_invalid.unsqueeze(0), float('-inf'))\n\n    return final_scores\n\ndef pytorch_rmsnorm(x: torch.Tensor, weight: torch.Tensor, eps: float = 1e-6, residual: Optional[torch.Tensor] = None) -> torch.Tensor:\n    if residual is not None:\n        x = x + residual\n    variance = x.pow(2).mean(-1, keepdim=True)\n    x_normed = x * torch.rsqrt(variance + eps)\n    return x_normed * weight\n\ndef _auto_chunk_size(batch_size: int, seq_kv: int, target_bytes: int = 512 * 1024 * 1024) -> int:\n    bytes_per_row = batch_size * seq_kv * 4\n    if bytes_per_row == 0: return 1\n    max_C = target_bytes // bytes_per_row\n    C = 1\n    while C * 2 <= max_C: C *= 2\n    return max(1, min(C, seq_kv))\n\ndef _compute_scores(q_chunk, k, w_chunk, b, scale, causal, q_offset, use_triton):\n    # Force PyTorch fallback for benchmark portability if triton missing\n    return pytorch_gated_indexer(q_chunk, k, w_chunk, b, scale, causal, q_offset)\n\ndef fused_indexer_topk(\n    q: torch.Tensor, k: torch.Tensor, w: torch.Tensor, b: torch.Tensor,\n    scale: float, causal: bool = True, k_base: int = 512, k_min: int = 32, k_max: int = 4096,\n    variance_ema: Optional[torch.Tensor] = None, is_training: bool = False, sink_size: int = 4\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Fused indexer + topk (PyTorch chunked version).\"\"\"\n    batch_size, seq_q, n_heads, d_idx = q.shape\n    seq_kv = k.shape[1]\n    device = q.device\n    \n    C = _auto_chunk_size(batch_size, seq_kv)\n    \n    # Heuristic: single-pass if output fits in memory\n    k_limit = min(seq_kv, max(k_max, sink_size))\n    \n    # Simple chunked implementation\n    var_t = torch.empty(batch_size, seq_q, device=device, dtype=torch.float32)\n    top_indices = torch.empty(batch_size, seq_q, k_limit, device=device, dtype=torch.int32)\n\n    for q_start in range(0, seq_q, C):\n        q_end = min(q_start + C, seq_q)\n        q_chunk = q[:, q_start:q_end]\n        w_chunk = w[:, q_start:q_end]\n\n        scores = _compute_scores(q_chunk, k, w_chunk, b, scale, causal, q_start, False)\n\n        # FIX BUG 2: Compute variance only over valid (non-masked) entries\n        valid_mask = scores != float('-inf')  # [batch, chunk, seq_kv]\n        valid_count = valid_mask.sum(dim=-1, keepdim=True).clamp(min=1)  # [batch, chunk, 1]\n        scores_zeroed = scores.masked_fill(~valid_mask, 0.0)\n        mean_valid = scores_zeroed.sum(dim=-1, keepdim=True) / valid_count\n        diff = (scores_zeroed - mean_valid * valid_mask.float())\n        diff = diff.masked_fill(~valid_mask, 0.0)\n        var_t[:, q_start:q_end] = (diff.pow(2).sum(dim=-1) / valid_count.squeeze(-1)).squeeze(-1)\n\n        # TopK\n        if seq_kv > sink_size:\n            scores = scores.clone()\n            scores[:, :, :sink_size] = float('inf')\n        \n        _, chunk_idx = scores.topk(k_limit, dim=-1)\n        top_indices[:, q_start:q_end, :] = chunk_idx.to(torch.int32)\n        del scores, chunk_idx\n\n    # Adaptive k_t computation\n    if variance_ema is not None:\n        avg_V = variance_ema.clamp(min=1e-6)\n    else:\n        avg_V = var_t.mean().clamp(min=1e-6)\n    k_t = (k_base * var_t / avg_V).floor().clamp(min=k_min, max=k_max).long()\n\n    return var_t, k_t, top_indices\n\ndef pytorch_sparse_attention(q, k, v, sparse_idx, sparse_mask, scale):\n    \"\"\"PyTorch fallback for sparse attention gathered by indices.\n    \n    Args:\n        q: [B, T, H, D] query tensor\n        k: [B, T, H, D] key tensor\n        v: [B, T, H, D] value tensor\n        sparse_idx: [B, H, T, K] indices of selected keys per query\n        sparse_mask: [B, H, T, K] mask (1=valid, 0=masked)\n        scale: attention scale factor\n    \n    Returns:\n        output: [B, T, H, D]\n    \"\"\"\n    B, T, H, D = q.shape\n    K = sparse_idx.shape[-1]\n    \n    # Transpose to [B, H, T, D] for head-first layout\n    q = q.transpose(1, 2).contiguous()\n    k = k.transpose(1, 2).contiguous()\n    v = v.transpose(1, 2).contiguous()\n    \n    # Clamp indices to valid range\n    idx = sparse_idx.clamp(0, T - 1).long()  # [B, H, T, K]\n    \n    # Gather keys and values using advanced indexing\n    # idx_exp: [B, H, T, K, D] for gathering from [B, H, T, D]\n    idx_exp = idx.unsqueeze(-1).expand(B, H, T, K, D)\n    \n    # Expand k, v for gather along the sequence dimension (dim=2)\n    k_gathered = torch.gather(k.unsqueeze(3).expand(-1, -1, -1, K, -1).contiguous().view(B, H, T * K, D),\n                              dim=2,\n                              index=idx_exp.contiguous().view(B, H, T * K, D)).view(B, H, T, K, D)\n    v_gathered = torch.gather(v.unsqueeze(3).expand(-1, -1, -1, K, -1).contiguous().view(B, H, T * K, D),\n                              dim=2,\n                              index=idx_exp.contiguous().view(B, H, T * K, D)).view(B, H, T, K, D)\n    \n    # Compute attention scores: [B, H, T, K]\n    scores = torch.einsum('bhtd,bhtkd->bhtk', q, k_gathered) * scale\n    \n    # Apply sparse mask (0 -> -inf)\n    scores = scores.masked_fill(sparse_mask == 0, float('-inf'))\n    \n    # Softmax over the K dimension\n    attn_weights = torch.softmax(scores, dim=-1)\n    # Replace NaN from all-masked rows with 0\n    attn_weights = attn_weights.nan_to_num(0.0)\n    \n    # Weighted sum of values: [B, H, T, D]\n    out = torch.einsum('bhtk,bhtkd->bhtd', attn_weights, v_gathered)\n    \n    # Transpose back to [B, T, H, D]\n    return out.transpose(1, 2).contiguous()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Core Components (Rotary, Norm, Kronecker)\n# ============================================================================\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return pytorch_rmsnorm(x, self.weight, self.eps)\n\nclass FusedRMSNormSwishGate(nn.Module):\n    def __init__(self, dim, eps=1e-6):\n        super().__init__()\n        self.norm = RMSNorm(dim, eps)\n\n    def forward(self, x, g):\n        x_norm = self.norm(x)\n        return g * F.silu(x_norm)\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim: int, max_position_embeddings: int = 8192, base: int = 10000,\n                 original_max_position_embeddings: int = 8192, scaling_factor: float = 32.0):\n        super().__init__()\n        self.dim = dim\n        self.base = base\n        self.max_position_embeddings = max_position_embeddings\n        self.scaling_factor = scaling_factor\n        \n        # Simple RoPE setup for benchmark\n        scaled_base = base\n        if max_position_embeddings > original_max_position_embeddings:\n            ext_ratio = max_position_embeddings / original_max_position_embeddings\n            scaled_base = base * (ext_ratio ** (dim / (dim - 2)))\n        \n        inv_freq = 1.0 / (scaled_base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        self.register_buffer(\"mscale\", torch.ones(dim // 2)) # Dummy mscale\n\n    def _compute_cos_sin(self, seq_len: int, device, dtype=None):\n        t = torch.arange(seq_len, device=device).float()\n        freqs = t.unsqueeze(-1) * self.inv_freq.unsqueeze(0)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        cos_out = emb.cos()\n        sin_out = emb.sin()\n        if dtype is not None:\n            cos_out = cos_out.to(dtype)\n            sin_out = sin_out.to(dtype)\n        return cos_out, sin_out\n\n    @staticmethod\n    def _apply_rotary(x, cos, sin):\n        # FIX BUG 4: Proper RoPE with interleaved output\n        # Split into first half and second half (paired rotation)\n        d = x.shape[-1]\n        x1 = x[..., :d//2]\n        x2 = x[..., d//2:]\n        cos_half = cos[..., :d//2]\n        sin_half = sin[..., :d//2]\n        # Apply 2D rotation to each pair and interleave back\n        o1 = x1 * cos_half - x2 * sin_half\n        o2 = x1 * sin_half + x2 * cos_half\n        # Stack and interleave: [o1_0, o2_0, o1_1, o2_1, ...]\n        return torch.stack((o1, o2), dim=-1).flatten(-2)\n\nclass ShortConvolution(nn.Module):\n    def __init__(self, dim, conv_size=4, activation='silu'):\n        super().__init__()\n        self.conv_size = conv_size\n        self.conv = nn.Conv1d(dim, dim, kernel_size=conv_size, padding=conv_size - 1, groups=dim)\n        self.activation = nn.SiLU() if activation == 'silu' else nn.Identity()\n\n    def forward(self, x):\n        x = x.transpose(1, 2)\n        x = self.conv(x)\n        x = x[:, :, :-(self.conv_size - 1)]\n        x = x.transpose(1, 2)\n        return self.activation(x)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Gated DeltaNet\n",
    "# ============================================================================\n",
    "\n",
    "class GatedDeltaNet(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, head_dim, max_seq_len=262144, \n",
    "                 rope_base=10000, rope_original_max=8192, rope_scaling_factor=32.0,\n",
    "                 conv_size=4, use_output_norm=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.use_output_norm = use_output_norm\n",
    "\n",
    "        key_dim = num_heads * head_dim\n",
    "        value_dim = num_heads * head_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, key_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(hidden_size, key_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(hidden_size, value_dim, bias=False)\n",
    "        self.g_proj = nn.Linear(hidden_size, value_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(value_dim, hidden_size, bias=False)\n",
    "\n",
    "        self.b_proj = nn.Linear(hidden_size, num_heads, bias=True)\n",
    "        self.gk_proj = nn.Linear(hidden_size, num_heads, bias=True)\n",
    "\n",
    "        self.q_conv1d = ShortConvolution(key_dim, conv_size=conv_size, activation='silu')\n",
    "        self.k_conv1d = ShortConvolution(key_dim, conv_size=conv_size, activation='silu')\n",
    "        self.v_conv1d = ShortConvolution(value_dim, conv_size=conv_size, activation='silu')\n",
    "\n",
    "        A_init = torch.empty(num_heads).uniform_(0, 16)\n",
    "        self.A_log = nn.Parameter(torch.log(A_init))\n",
    "        self.D = nn.Parameter(torch.ones(num_heads))\n",
    "        self.dt_bias = nn.Parameter(torch.rand(num_heads) * 0.02 - 0.01)\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(head_dim, max_seq_len)\n",
    "\n",
    "        if use_output_norm:\n",
    "            self.o_norm = FusedRMSNormSwishGate(head_dim)\n",
    "\n",
    "    def _delta_rule_python(self, q, k, v, alpha, beta, B, T, device, original_dtype):\n",
    "        # Transpose\n",
    "        q_h = q.transpose(1, 2)\n",
    "        k_h = k.transpose(1, 2)\n",
    "        v_h = v.transpose(1, 2)\n",
    "        beta_h = beta.transpose(1, 2)\n",
    "        alpha_h = alpha.transpose(1, 2)\n",
    "\n",
    "        S = torch.zeros(B, self.num_heads, self.head_dim, self.head_dim, device=device, dtype=torch.float32)\n",
    "        outputs = torch.empty(B, self.num_heads, T, self.head_dim, device=device, dtype=torch.float32)\n",
    "        I = torch.eye(self.head_dim, device=device, dtype=torch.float32).view(1, 1, self.head_dim, self.head_dim)\n",
    "\n",
    "        # Basic recurrence loop\n",
    "        for t in range(T):\n",
    "            q_t = q_h[:, :, t, :].float()\n",
    "            k_t = k_h[:, :, t, :].float()\n",
    "            v_t = v_h[:, :, t, :].float()\n",
    "            beta_t = beta_h[:, :, t, 0].float()\n",
    "            alpha_t = alpha_h[:, :, t, 0].float()\n",
    "\n",
    "            o_t = torch.einsum('bhd,bhde->bhe', q_t, S)\n",
    "            o_t = o_t + self.D.view(1, self.num_heads, 1) * (q_t * k_t).sum(dim=-1, keepdim=True) * v_t\n",
    "            outputs[:, :, t, :] = o_t\n",
    "\n",
    "            v_outer = torch.einsum('bhd,bhe->bhde', v_t, k_t)\n",
    "            k_outer = torch.einsum('bhd,bhe->bhde', k_t, k_t)\n",
    "            alpha_t = alpha_t.view(B, self.num_heads, 1, 1)\n",
    "            beta_t = beta_t.view(B, self.num_heads, 1, 1)\n",
    "\n",
    "            orthogonal_proj = I - beta_t * k_outer\n",
    "            S = alpha_t * torch.einsum('bhde,bhef->bhdf', S, orthogonal_proj) + beta_t * v_outer\n",
    "\n",
    "        return outputs.to(original_dtype).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        g = self.g_proj(x)\n",
    "\n",
    "        q = self.q_conv1d(q)\n",
    "        k = self.k_conv1d(k)\n",
    "        v = self.v_conv1d(v)\n",
    "\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim)\n",
    "        g = g.view(B, T, self.num_heads, self.head_dim)\n",
    "\n",
    "        cos, sin = self.rotary_emb._compute_cos_sin(T, device, x.dtype)\n",
    "        cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "        sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "        q = self.rotary_emb._apply_rotary(q, cos, sin)\n",
    "        k = self.rotary_emb._apply_rotary(k, cos, sin)\n",
    "\n",
    "        q = F.normalize(q, p=2, dim=-1)\n",
    "        k = F.normalize(k, p=2, dim=-1)\n",
    "\n",
    "        beta = torch.sigmoid(self.b_proj(x)).unsqueeze(-1)\n",
    "        gk = self.gk_proj(x)\n",
    "        A = torch.exp(self.A_log)\n",
    "        alpha = torch.exp(-A.view(1, 1, self.num_heads, 1) * F.softplus(gk + self.dt_bias).unsqueeze(-1))\n",
    "\n",
    "        # Use Python fallback if no Triton/fla\n",
    "        o = self._delta_rule_python(q, k, v, alpha, beta, B, T, device, x.dtype)\n",
    "\n",
    "        if self.use_output_norm:\n",
    "            o_flat = o.reshape(B * T * self.num_heads, self.head_dim)\n",
    "            g_flat = g.reshape(B * T * self.num_heads, self.head_dim)\n",
    "            o_normed = self.o_norm(o_flat, g_flat)\n",
    "            o = o_normed.view(B, T, self.num_heads, self.head_dim)\n",
    "        \n",
    "        o = o.reshape(B, T, self.num_heads * self.head_dim)\n",
    "        return self.o_proj(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Gated Sparse Attention (GSA)\n# ============================================================================\n\nclass GatedSparseAttention(nn.Module):\n    def __init__(self, hidden_size, num_heads, max_seq_len=262144, \n                 k_base=512, k_min=32, k_max=1024, indexer_heads=4):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n        \n        self.k_base = k_base\n        self.k_min = k_min\n        self.k_max = k_max\n        self.indexer_heads = indexer_heads\n        \n        self.d_idx = 32\n        self.W_Iq = nn.Linear(hidden_size, indexer_heads * self.d_idx, bias=False)\n        self.W_Ik = nn.Linear(hidden_size, self.d_idx, bias=False)\n        self.W_Iw = nn.Linear(hidden_size, indexer_heads, bias=False)\n        self.gate_bias = nn.Parameter(torch.zeros(indexer_heads))\n        \n        self.register_buffer(\"variance_ema\", torch.tensor(1.0))\n        self.ema_momentum = 0.1\n        \n        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.W_v = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n        \n        self.W_gv = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.W_go = nn.Linear(hidden_size, hidden_size, bias=False)\n        \n        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n\n    def forward(self, x, attention_mask=None):\n        B, T, C = x.shape\n        device = x.device\n        \n        # Indexer\n        q_I = self.W_Iq(x).view(B, T, self.indexer_heads, self.d_idx)\n        k_I = self.W_Ik(x)\n        w_raw = self.W_Iw(x)\n        scale_idx = 1.0 / math.sqrt(self.d_idx)\n        \n        # FIX BUG 5: Pass variance_ema to fused_indexer_topk\n        var_t, k_t, top_indices = fused_indexer_topk(\n            q=q_I, k=k_I, w=w_raw, b=self.gate_bias, scale=scale_idx, \n            causal=True, k_base=self.k_base, k_min=self.k_min, k_max=self.k_max,\n            variance_ema=self.variance_ema, is_training=self.training\n        )\n        \n        # Update EMA during training\n        if self.training:\n            with torch.no_grad():\n                batch_var_mean = var_t.mean()\n                self.variance_ema.mul_(1.0 - self.ema_momentum).add_(self.ema_momentum * batch_var_mean)\n        \n        k_limit = top_indices.size(-1)\n        base_idx = top_indices.long()\n        range_k = torch.arange(k_limit, device=device)\n        keep_mask = range_k.view(1, 1, -1) < k_t.unsqueeze(-1)\n        \n        # Attention\n        q = self.W_q(x)\n        k_attn = self.W_k(x)\n        v = self.W_v(x)\n        \n        g_v = torch.sigmoid(self.W_gv(x))\n        v = v * g_v\n        \n        q = q.view(B, T, self.num_heads, self.head_dim)\n        k_attn = k_attn.view(B, T, self.num_heads, self.head_dim)\n        v = v.view(B, T, self.num_heads, self.head_dim)\n        \n        cos, sin = self.rotary_emb._compute_cos_sin(T, device, x.dtype)\n        cos = cos.unsqueeze(0).unsqueeze(2)\n        sin = sin.unsqueeze(0).unsqueeze(2)\n        q = self.rotary_emb._apply_rotary(q, cos, sin)\n        k_attn = self.rotary_emb._apply_rotary(k_attn, cos, sin)\n        \n        # Sparse Attention Call\n        sparse_idx = base_idx.unsqueeze(1).expand(B, self.num_heads, T, k_limit)\n        sparse_mask = keep_mask.float().unsqueeze(1).expand(B, self.num_heads, T, k_limit)\n        scale_attn = 1.0 / math.sqrt(self.head_dim)\n        \n        o_sparse = pytorch_sparse_attention(q, k_attn, v, sparse_idx, sparse_mask, scale_attn)\n        o_sparse = o_sparse.contiguous().view(B, T, self.hidden_size)\n        \n        g_o = torch.sigmoid(self.W_go(x))\n        return self.o_proj(o_sparse * g_o)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Benchmarking Infrastructure\n# ============================================================================\n\ndef benchmark_run(model_cls, name, configs, device, verbose=True):\n    results = []\n    \n    print(f\"Benchmarking {name}...\")\n    print(f\"{'B':<4} {'T':<6} {'D':<6} | {'Time (ms)':<10} | {'Tokens/s':<10} | {'Mem (MB)':<10}\")\n    print(\"-\" * 60)\n    \n    for (B, T, D) in configs:\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        \n        num_heads = 16\n        head_dim = D // num_heads\n        \n        # Instantiate model\n        try:\n            if name == \"GatedDeltaNet\":\n                model = model_cls(D, num_heads, head_dim).to(device).to(torch.bfloat16)\n            else:\n                model = model_cls(D, num_heads).to(device).to(torch.bfloat16)\n            \n            model.eval()\n            x = torch.randn(B, T, D, device=device, dtype=torch.bfloat16)\n            \n            # Warmup (gradients already globally disabled via torch.set_grad_enabled(False))\n            for _ in range(5):\n                _ = model(x)\n            \n            torch.cuda.synchronize()\n            start_event = torch.cuda.Event(enable_timing=True)\n            end_event = torch.cuda.Event(enable_timing=True)\n            \n            start_event.record()\n            for _ in range(10):\n                _ = model(x)\n            end_event.record()\n            torch.cuda.synchronize()\n            \n            elapsed_ms = start_event.elapsed_time(end_event) / 10.0\n            tokens_per_sec = (B * T) / (elapsed_ms / 1000.0)\n            mem_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n            \n            print(f\"{B:<4} {T:<6} {D:<6} | {elapsed_ms:<10.2f} | {tokens_per_sec:<10.0f} | {mem_mb:<10.0f}\")\n            \n            results.append({\n                \"B\": B, \"T\": T, \"D\": D,\n                \"time_ms\": elapsed_ms,\n                \"throughput\": tokens_per_sec,\n                \"memory_mb\": mem_mb\n            })\n            \n            del model, x\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"{B:<4} {T:<6} {D:<6} | {'OOM':<10} | {'-':<10} | {'-':<10}\")\n            else:\n                print(f\"{B:<4} {T:<6} {D:<6} | {'ERROR':<10} | {'-':<10} | {str(e)[:10]}\")\n    \n    return results\n\ndef plot_results(results_delta, results_gsa):\n    # Extract data for T scaling (assuming B=1, D=fixed)\n    ts = sorted(list(set(r[\"T\"] for r in results_delta)))\n    \n    # Throughput\n    tp_delta = [next((r[\"throughput\"] for r in results_delta if r[\"T\"] == t), 0) for t in ts]\n    tp_gsa = [next((r[\"throughput\"] for r in results_gsa if r[\"T\"] == t), 0) for t in ts]\n    \n    plt.figure(figsize=(10, 5))\n    plt.plot(ts, tp_delta, marker='o', label='DeltaNet')\n    plt.plot(ts, tp_gsa, marker='x', label='GSA')\n    plt.xlabel(\"Sequence Length\")\n    plt.ylabel(\"Tokens / Sec\")\n    plt.title(\"Throughput vs Sequence Length\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    # Memory\n    mem_delta = [next((r[\"memory_mb\"] for r in results_delta if r[\"T\"] == t), 0) for t in ts]\n    mem_gsa = [next((r[\"memory_mb\"] for r in results_gsa if r[\"T\"] == t), 0) for t in ts]\n    \n    plt.figure(figsize=(10, 5))\n    plt.plot(ts, mem_delta, marker='o', label='DeltaNet')\n    plt.plot(ts, mem_gsa, marker='x', label='GSA')\n    plt.xlabel(\"Sequence Length\")\n    plt.ylabel(\"Peak Memory (MB)\")\n    plt.title(\"Memory vs Sequence Length\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run Benchmarks\n",
    "# Define configurations: [(B, T, D)]\n",
    "configs = [\n",
    "    (1, 1024, 2048),\n",
    "    (1, 2048, 2048),\n",
    "    (1, 4096, 2048),\n",
    "    (1, 8192, 2048),\n",
    "    # Uncomment for larger runs if GPU permits (PyTorch fallback is slow!)\n",
    "    # (1, 16384, 2048),\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    results_delta = benchmark_run(GatedDeltaNet, \"GatedDeltaNet\", configs, device)\n",
    "    print(\"\\n\")\n",
    "    results_gsa = benchmark_run(GatedSparseAttention, \"GatedSparseAttention\", configs, device)\n",
    "    \n",
    "    plot_results(results_delta, results_gsa)\n",
    "else:\n",
    "    print(\"Skipping benchmark execution (requires CUDA for timing/memory).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}