{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Sparse Attention (GSA) — Step-by-Step Explanation\n",
    "\n",
    "**Reference:** [alfredcs/Gated-Sparse-Attention](https://github.com/alfredcs/Gated-Sparse-Attention)\n",
    "\n",
    "This notebook walks through the entire GSA forward pass using a **tiny example** so you can inspect every tensor at every step.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Input x [B, T, D]\n",
    "  │\n",
    "  ├──► q_proj ──► reshape [B,T,H,d] ──► RoPE ──────────────────────────┐\n",
    "  ├──► k_proj ──► reshape [B,T,H,d] ──► RoPE ──────────────────────────┤\n",
    "  ├──► v_proj ──► reshape [B,T,H,d] ──► ValueGate(G2) ────────────────┤\n",
    "  │                                                                     │\n",
    "  ├──► GatedLightningIndexer ──► scores [B,T,T] ──┐                    │\n",
    "  │                                                 │                    │\n",
    "  │                              AdaptiveTopKSelector│                   │\n",
    "  │                              indices [B,T,k] ───┼──► Sparse ────────┤\n",
    "  │                              mask [B,T,k] ──────┘    Attention      │\n",
    "  │                                                       │             │\n",
    "  │                                    attn_output ◄──────┘             │\n",
    "  │                                        │                            │\n",
    "  ├──► OutputGate(G1) ─────────────────────┘                            │\n",
    "  │        │                                                            │\n",
    "  │   o_proj ──► Output                                                 │\n",
    "```\n",
    "\n",
    "### Key Idea: Sparse Attention via Learned Indexing\n",
    "\n",
    "Instead of attending to ALL tokens (O(T²)), GSA:\n",
    "1. Uses a **lightweight indexer** to score all token pairs → O(T²) with tiny d_indexer\n",
    "2. Selects **top-k most important** tokens per query → reduces to O(T×k)\n",
    "3. Performs **full attention** only on the selected subset\n",
    "4. Uses **dual gating** (value + output gates) for fine-grained control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Tiny example dimensions\n",
    "B = 1        # batch size\n",
    "T = 8        # sequence length (small enough to see full T×T matrices)\n",
    "D = 32       # model dimension\n",
    "H = 2        # attention heads\n",
    "d_head = D // H  # 16 per head\n",
    "\n",
    "# Indexer dimensions\n",
    "d_indexer = 8      # indexer dimension (ref uses 64, we use 8 for visibility)\n",
    "n_idx_heads = 2    # indexer heads (ref uses 4)\n",
    "k_select = 4       # number of tokens to attend to (ref uses adaptive ~2048)\n",
    "\n",
    "print(f\"Model: D={D}, H={H}, d_head={d_head}\")\n",
    "print(f\"Indexer: d_indexer={d_indexer}, n_idx_heads={n_idx_heads}\")\n",
    "print(f\"Sparse selection: k={k_select} out of T={T} tokens\")\n",
    "print(f\"\\nSparsity ratio: {k_select}/{T} = {k_select/T:.1%} of tokens attended\")\n",
    "print(f\"For real models at T=4096, k=2048: 50% sparsity\")\n",
    "print(f\"For real models at T=8192, k=2048: 25% sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Input and QKV Projections\n",
    "\n",
    "Standard attention projections. Unlike GDN, GSA uses symmetric dimensions (Q, K, V all have dim D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "x = torch.randn(B, T, D)\n",
    "print(f\"Input x shape: {x.shape}\")\n",
    "\n",
    "# Standard QKV projections\n",
    "q_proj = nn.Linear(D, D, bias=False)\n",
    "k_proj = nn.Linear(D, D, bias=False)\n",
    "v_proj = nn.Linear(D, D, bias=False)\n",
    "\n",
    "q = q_proj(x).view(B, T, H, d_head)  # [1, 8, 2, 16]\n",
    "k = k_proj(x).view(B, T, H, d_head)  # [1, 8, 2, 16]\n",
    "v = v_proj(x).view(B, T, H, d_head)  # [1, 8, 2, 16]\n",
    "\n",
    "print(f\"\\nQ shape: {q.shape}\")\n",
    "print(f\"K shape: {k.shape}\")\n",
    "print(f\"V shape: {v.shape}\")\n",
    "print(f\"\\nAll projections have the same dimension D={D} (symmetric)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Value Gate (G2)\n",
    "\n",
    "The **Value Gate** modulates values *before* attention:\n",
    "\n",
    "```\n",
    "v_gated = v * sigmoid(W_gv @ x + b_gv)\n",
    "```\n",
    "\n",
    "This allows the model to suppress or amplify value vectors based on the input context.\n",
    "The bias is initialized to 0.5, so `sigmoid(0.5) ≈ 0.62` — values are slightly dampened by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Gate (G2)\n",
    "value_gate_proj = nn.Linear(D, D, bias=True)\n",
    "nn.init.constant_(value_gate_proj.bias, 0.5)  # ref: bias_init=0.5\n",
    "\n",
    "# Compute gate\n",
    "gate_logits = value_gate_proj(x)  # [B, T, D]\n",
    "value_gate = torch.sigmoid(gate_logits).view(B, T, H, d_head)\n",
    "\n",
    "print(f\"Value gate shape: {value_gate.shape}\")\n",
    "print(f\"\\nGate values (head 0, first 4 tokens, first 4 dims):\")\n",
    "print(value_gate[0, :4, 0, :4].numpy().round(3))\n",
    "\n",
    "print(f\"\\nMean gate value: {value_gate.mean():.3f} (initialized near sigmoid(0.5) ≈ 0.622)\")\n",
    "\n",
    "# Apply gate\n",
    "v_before_gate = v.clone()\n",
    "v = v * value_gate\n",
    "\n",
    "print(f\"\\nV norms before gate: {v_before_gate[0, :, 0].norm(dim=-1).numpy().round(3)}\")\n",
    "print(f\"V norms after  gate: {v[0, :, 0].norm(dim=-1).numpy().round(3)}\")\n",
    "print(f\"\\nGate dampens values → prevents attention from distributing too much weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Rotary Position Embeddings (RoPE)\n",
    "\n",
    "Same as GDN — standard RoPE with `rotate_half`. Applied to Q and K (not V)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RoPE\n",
    "base = 10000.0\n",
    "inv_freq = 1.0 / (base ** (torch.arange(0, d_head, 2).float() / d_head))\n",
    "t = torch.arange(T, dtype=torch.float32)\n",
    "freqs = torch.outer(t, inv_freq)\n",
    "emb = torch.cat((freqs, freqs), dim=-1)\n",
    "cos = emb.cos().unsqueeze(0).unsqueeze(2)  # [1, T, 1, d_head]\n",
    "sin = emb.sin().unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., :x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "q = q * cos + rotate_half(q) * sin\n",
    "k = k * cos + rotate_half(k) * sin\n",
    "\n",
    "print(f\"RoPE applied to Q and K (not V).\")\n",
    "print(f\"Q shape after RoPE: {q.shape}\")\n",
    "print(f\"K shape after RoPE: {k.shape}\")\n",
    "print(f\"V shape (unchanged): {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Gated Lightning Indexer\n",
    "\n",
    "This is the **core innovation of GSA**. The indexer computes importance scores for every (query, key) pair using a *cheap, separate* set of projections.\n",
    "\n",
    "### Formula\n",
    "\n",
    "```\n",
    "I(t, s) = Σ_h  σ(w_t^h) × σ(q_I^h · k_I_s / √d_idx + b^h)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `q_I`: indexer queries [B, T, n_idx_heads, d_indexer] — separate from attention Q!\n",
    "- `k_I`: indexer keys [B, T, d_indexer] — shared across indexer heads\n",
    "- `w`: importance weights [B, T, n_idx_heads] — query-dependent\n",
    "- `b`: learnable bias per indexer head\n",
    "\n",
    "**Why is this efficient?** The indexer uses `d_indexer=64` (vs `d_head=128`), so the O(T²) scoring is done in a much cheaper space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexer projections\n",
    "idx_q_proj = nn.Linear(D, n_idx_heads * d_indexer, bias=False)  # -> [B, T, n_idx_heads * d_idx]\n",
    "idx_k_proj = nn.Linear(D, d_indexer, bias=False)                # -> [B, T, d_idx] (shared)\n",
    "idx_w_proj = nn.Linear(D, n_idx_heads, bias=True)               # -> [B, T, n_idx_heads]\n",
    "idx_bias = nn.Parameter(torch.zeros(n_idx_heads))                # [n_idx_heads]\n",
    "\n",
    "# Xavier init with specific gains (ref)\n",
    "nn.init.xavier_uniform_(idx_q_proj.weight, gain=1.0)\n",
    "nn.init.xavier_uniform_(idx_k_proj.weight, gain=1.0)\n",
    "nn.init.xavier_uniform_(idx_w_proj.weight, gain=0.1)  # Small gain for weights\n",
    "\n",
    "print(f\"Indexer projections:\")\n",
    "print(f\"  q_I: [{D}] → [{n_idx_heads} × {d_indexer}] = {n_idx_heads * d_indexer}\")\n",
    "print(f\"  k_I: [{D}] → [{d_indexer}] (shared across indexer heads)\")\n",
    "print(f\"  w:   [{D}] → [{n_idx_heads}] (importance weights)\")\n",
    "print(f\"  b:   [{n_idx_heads}] (learnable bias)\")\n",
    "print(f\"\\nFLOP cost: ~{2 * T * T * d_indexer * n_idx_heads:,} for T²×d_idx scoring\")\n",
    "print(f\"vs attention: ~{2 * T * T * d_head * H:,} for T²×d_head×H\")\n",
    "print(f\"Indexer is {d_indexer * n_idx_heads / (d_head * H):.1%} the cost of full attention QK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute indexer scores step by step\n",
    "scale_idx = 1.0 / math.sqrt(d_indexer)\n",
    "\n",
    "# Project to indexer space\n",
    "q_idx = idx_q_proj(x).view(B, T, n_idx_heads, d_indexer)  # [1, 8, 2, 8]\n",
    "k_idx = idx_k_proj(x)  # [1, 8, 8] — shared across heads!\n",
    "\n",
    "print(f\"Indexer Q shape: {q_idx.shape} = [B, T, n_idx_heads, d_indexer]\")\n",
    "print(f\"Indexer K shape: {k_idx.shape} = [B, T, d_indexer] (shared!)\")\n",
    "\n",
    "# Raw dot products per indexer head: [B, n_heads, T_q, T_kv]\n",
    "raw_scores = torch.einsum('bqhd,bkd->bhqk', q_idx.float(), k_idx.float()) * scale_idx\n",
    "print(f\"\\nRaw score matrix shape: {raw_scores.shape} = [B, n_idx_heads, T, T]\")\n",
    "print(f\"\\nRaw scores (head 0):\")\n",
    "print(raw_scores[0, 0].numpy().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid gating with learnable bias\n",
    "bias_exp = idx_bias.float().view(1, -1, 1, 1)\n",
    "gated_scores = torch.sigmoid(raw_scores + bias_exp)\n",
    "\n",
    "print(\"Sigmoid gating: σ(raw_score + bias)\")\n",
    "print(f\"Bias values: {idx_bias.data.numpy().round(3)}\")\n",
    "print(f\"\\nGated scores (head 0) — values in (0, 1):\")\n",
    "print(gated_scores[0, 0].numpy().round(3))\n",
    "\n",
    "# Importance weights: sigmoid(w_proj(x))\n",
    "w = torch.sigmoid(idx_w_proj(x).float())  # [B, T, n_idx_heads]\n",
    "w_exp = w.permute(0, 2, 1).unsqueeze(-1)  # [B, n_heads, T, 1]\n",
    "\n",
    "print(f\"\\nImportance weights (per query position, per head):\")\n",
    "print(w[0].numpy().round(3))\n",
    "\n",
    "# Weighted sum across indexer heads → final scores\n",
    "weighted = gated_scores * w_exp\n",
    "final_scores = weighted.sum(dim=1)  # [B, T, T]\n",
    "\n",
    "# Apply causal mask\n",
    "causal_mask = torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)\n",
    "final_scores = final_scores.masked_fill(causal_mask.unsqueeze(0), float('-inf'))\n",
    "\n",
    "print(f\"\\nFinal indexer scores [B, T, T] (with causal mask):\")\n",
    "print(final_scores[0].numpy().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Adaptive Top-K Selection\n",
    "\n",
    "From the T×T score matrix, we select the **top-k** most important tokens for each query.\n",
    "\n",
    "The reference uses **variance-based adaptive k**: higher variance in scores → more focused attention → fewer tokens needed.\n",
    "\n",
    "For our tiny example, we'll use a fixed k=4 out of 8 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K selection\n",
    "k_effective = min(k_select, T)  # 4\n",
    "\n",
    "# Replace -inf with large negative for topk\n",
    "scores_for_topk = final_scores.masked_fill(final_scores == float('-inf'), -1e9)\n",
    "\n",
    "# Select top-k indices per query position\n",
    "topk_values, indices = torch.topk(scores_for_topk, k_effective, dim=-1)\n",
    "\n",
    "print(f\"Top-{k_effective} selection from {T} tokens per query:\")\n",
    "print(f\"\\nIndices shape: {indices.shape} = [B, T, k_select]\")\n",
    "print(f\"\\nSelected token indices per query position:\")\n",
    "for t_pos in range(T):\n",
    "    idx = indices[0, t_pos].numpy()\n",
    "    vals = topk_values[0, t_pos].numpy().round(3)\n",
    "    print(f\"  Query t={t_pos}: indices={idx}, scores={vals}\")\n",
    "\n",
    "# Build validity mask\n",
    "# In practice, causal constraint means early positions have fewer valid keys\n",
    "gathered_scores = torch.gather(final_scores, -1, indices)\n",
    "mask = gathered_scores != float('-inf')\n",
    "\n",
    "print(f\"\\nValidity mask (True=valid token):\")\n",
    "for t_pos in range(T):\n",
    "    print(f\"  Query t={t_pos}: {mask[0, t_pos].numpy()}\")\n",
    "print(f\"\\nNote: Early positions have fewer valid tokens due to causal constraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sparse Attention with Gather\n",
    "\n",
    "Now we perform **standard attention** but only on the selected k tokens.\n",
    "\n",
    "### The Gather Operation\n",
    "\n",
    "The key trick is `torch.gather` to fetch only the selected K/V vectors:\n",
    "\n",
    "```python\n",
    "# indices: [B, T, k]  (which tokens to attend to)\n",
    "# K, V:    [B, T_kv, H, D]\n",
    "\n",
    "# 1. Expand: [B, T_kv, H, D] → [B, T, T_kv, H, D]  (repeat for each query)\n",
    "# 2. Gather: pick k entries from T_kv → [B, T, k, H, D]\n",
    "# 3. Attend: softmax(Q @ K_gathered^T / √d) @ V_gathered\n",
    "```\n",
    "\n",
    "The indices are **shared across attention heads** — same tokens selected for all heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Attention — step by step\n",
    "scale_attn = 1.0 / math.sqrt(d_head)\n",
    "\n",
    "# Clamp indices to valid range\n",
    "idx = indices.clamp(0, T - 1).long()  # [B, T, k_select]\n",
    "\n",
    "print(\"Step 6a: Gather K and V using selected indices\")\n",
    "print(f\"  K shape: {k.shape} = [B, T_kv, H, D]\")\n",
    "print(f\"  indices shape: {idx.shape} = [B, T, k_select]\")\n",
    "\n",
    "# Expand indices for gather: [B, T, k] → [B, T, k, H, D]\n",
    "idx_exp = idx.unsqueeze(-1).unsqueeze(-1).expand(B, T, k_effective, H, d_head)\n",
    "\n",
    "# Expand K, V: [B, T_kv, H, D] → [B, T, T_kv, H, D]\n",
    "k_expanded = k.unsqueeze(1).expand(B, T, T, H, d_head)\n",
    "v_expanded = v.unsqueeze(1).expand(B, T, T, H, d_head)\n",
    "\n",
    "# Gather selected tokens\n",
    "k_gathered = torch.gather(k_expanded, 2, idx_exp)  # [B, T, k, H, D]\n",
    "v_gathered = torch.gather(v_expanded, 2, idx_exp)  # [B, T, k, H, D]\n",
    "\n",
    "print(f\"  K gathered: {k_gathered.shape} = [B, T, k_select, H, D]\")\n",
    "print(f\"  V gathered: {v_gathered.shape} = [B, T, k_select, H, D]\")\n",
    "\n",
    "# Permute for attention: [B, T, H, k, D]\n",
    "k_gathered = k_gathered.permute(0, 1, 3, 2, 4)  # [B, T, H, k, D]\n",
    "v_gathered = v_gathered.permute(0, 1, 3, 2, 4)\n",
    "print(f\"  Permuted for attention: {k_gathered.shape} = [B, T, H, k_select, D]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention scores\n",
    "print(\"Step 6b: Compute attention scores on selected tokens\")\n",
    "\n",
    "# Q: [B, T, H, D] @ K_gathered: [B, T, H, k, D]^T → [B, T, H, k]\n",
    "attn_scores = torch.einsum('bqhd,bqhkd->bqhk', q, k_gathered) * scale_attn\n",
    "print(f\"  Attention scores shape: {attn_scores.shape} = [B, T, H, k_select]\")\n",
    "\n",
    "# Apply mask: [B, T, k] → [B, T, 1, k]\n",
    "mask_exp = mask.unsqueeze(2)  # broadcast across heads\n",
    "attn_scores = attn_scores.masked_fill(~mask_exp, float('-inf'))\n",
    "\n",
    "# Softmax over k_select dimension\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "attn_weights = attn_weights.masked_fill(~mask_exp, 0.0)\n",
    "attn_weights = attn_weights.nan_to_num(0.0)\n",
    "\n",
    "print(f\"\\nAttention weights (query=3, head=0) — sums to 1.0 over selected tokens:\")\n",
    "w = attn_weights[0, 3, 0].numpy().round(3)\n",
    "idx_at_3 = indices[0, 3].numpy()\n",
    "for j in range(k_effective):\n",
    "    print(f\"  Token {idx_at_3[j]}: weight={w[j]:.3f}  {'(masked)' if w[j] == 0 else ''}\")\n",
    "print(f\"  Sum: {w.sum():.3f}\")\n",
    "\n",
    "# Weighted sum: [B, T, H, k] @ [B, T, H, k, D] → [B, T, H, D]\n",
    "attn_output = torch.einsum('bqhk,bqhkd->bqhd', attn_weights, v_gathered)\n",
    "print(f\"\\nAttention output shape: {attn_output.shape} = [B, T, H, D]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Output Gate (G1)\n",
    "\n",
    "After sparse attention, the output is modulated by the **Output Gate**:\n",
    "\n",
    "```\n",
    "o_gated = attn_output * sigmoid(W_go @ x + b_go)\n",
    "```\n",
    "\n",
    "This provides a second level of control — the model can learn to suppress the attention output for certain positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Gate (G1)\n",
    "output_gate_proj = nn.Linear(D, D, bias=True)\n",
    "nn.init.constant_(output_gate_proj.bias, 0.5)\n",
    "\n",
    "output_gate = torch.sigmoid(output_gate_proj(x)).view(B, T, H, d_head)\n",
    "\n",
    "print(f\"Output gate shape: {output_gate.shape}\")\n",
    "print(f\"Mean gate value: {output_gate.mean():.3f}\")\n",
    "\n",
    "# Apply gate\n",
    "attn_before_gate = attn_output.clone()\n",
    "attn_output = attn_output * output_gate\n",
    "\n",
    "print(f\"\\nOutput norms before gate: {attn_before_gate[0, :, 0].norm(dim=-1).numpy().round(3)}\")\n",
    "print(f\"Output norms after  gate: {attn_output[0, :, 0].norm(dim=-1).numpy().round(3)}\")\n",
    "\n",
    "print(f\"\\nDual gating summary:\")\n",
    "print(f\"  G2 (ValueGate):  v = v * σ(W_gv·x + b)  — controls what info values carry\")\n",
    "print(f\"  G1 (OutputGate): o = o * σ(W_go·x + b)   — controls what info to output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Output Projection\n",
    "\n",
    "Reshape heads back together and project to model dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and project\n",
    "attn_output = attn_output.reshape(B, T, D)\n",
    "o_proj = nn.Linear(D, D, bias=False)\n",
    "final_output = o_proj(attn_output)\n",
    "\n",
    "print(f\"Final output shape: {final_output.shape} = [B, T, D]\")\n",
    "print(f\"\\nMatches input shape — GSA is a drop-in replacement for standard attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Sparse vs Full Attention\n",
    "\n",
    "Let's verify: does sparse attention give a reasonable approximation of full attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full causal attention for comparison\n",
    "def full_causal_attention(q, k, v, scale):\n",
    "    \"\"\"Standard full causal attention: [B, T, H, D]\"\"\"\n",
    "    scores = torch.einsum('bqhd,bkhd->bhqk', q, k) * scale\n",
    "    causal = torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)\n",
    "    scores = scores.masked_fill(causal.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    return torch.einsum('bhqk,bkhd->bqhd', weights, v), weights\n",
    "\n",
    "full_output, full_weights = full_causal_attention(q, k, v, scale_attn)\n",
    "\n",
    "print(f\"Full attention output shape: {full_output.shape}\")\n",
    "print(f\"Sparse attention output shape: {attn_before_gate.shape}\")\n",
    "\n",
    "# Compare\n",
    "diff = (full_output - attn_before_gate).abs()\n",
    "print(f\"\\nMean absolute difference: {diff.mean():.4f}\")\n",
    "print(f\"Max absolute difference: {diff.max():.4f}\")\n",
    "\n",
    "# Show which tokens full attention focuses on vs sparse\n",
    "print(f\"\\nFull attention weights (query=5, head=0):\")\n",
    "fw = full_weights[0, 0, 5].numpy().round(3)\n",
    "for j in range(T):\n",
    "    selected = '  ◄ SELECTED' if j in indices[0, 5].numpy() else ''\n",
    "    print(f\"  Token {j}: weight={fw[j]:.3f}{selected}\")\n",
    "print(f\"\\nThe indexer should learn to select the tokens with highest attention weight!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Top-K: How It Works\n",
    "\n",
    "The reference uses **variance-based** adaptive k:\n",
    "\n",
    "- High variance in indexer scores → attention is very focused → fewer tokens suffice\n",
    "- Low variance → attention is spread out → more tokens needed\n",
    "\n",
    "```\n",
    "k_adaptive = k_base × avg_variance / position_variance\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate adaptive k computation\n",
    "k_base = 4  # base number of tokens\n",
    "k_min = 2\n",
    "k_max = 6\n",
    "\n",
    "# Use the indexer scores\n",
    "valid_mask = final_scores != float('-inf')\n",
    "valid_count = valid_mask.sum(dim=-1, keepdim=True).clamp(min=1).float()\n",
    "scores_valid = final_scores.masked_fill(~valid_mask, 0.0)\n",
    "\n",
    "# Per-position variance\n",
    "mean = scores_valid.sum(dim=-1, keepdim=True) / valid_count\n",
    "diff = (scores_valid - mean * valid_mask.float()).masked_fill(~valid_mask, 0.0)\n",
    "variance = diff.pow(2).sum(dim=-1) / valid_count.squeeze(-1)\n",
    "\n",
    "avg_var = variance.mean().clamp(min=1e-6)\n",
    "\n",
    "# k_adaptive = k_base × avg_var / pos_var\n",
    "k_adaptive = (k_base * avg_var / variance.clamp(min=1e-6)).floor().clamp(min=k_min, max=k_max).long()\n",
    "\n",
    "print(f\"Per-position variance of indexer scores:\")\n",
    "print(f\"  {variance[0].numpy().round(4)}\")\n",
    "print(f\"\\nAverage variance: {avg_var.item():.4f}\")\n",
    "print(f\"\\nAdaptive k per query position:\")\n",
    "print(f\"  {k_adaptive[0].numpy()}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "for t_pos in range(T):\n",
    "    var = variance[0, t_pos].item()\n",
    "    k_val = k_adaptive[0, t_pos].item()\n",
    "    focus = 'focused' if var > avg_var.item() else 'spread'\n",
    "    print(f\"  t={t_pos}: var={var:.4f} ({focus}) → k={k_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Sparse Attention Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Full attention weights\n",
    "fw_np = full_weights[0, 0].numpy()\n",
    "im1 = axes[0].imshow(fw_np, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title('Full Causal Attention\\n(Head 0)')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# 2. Indexer scores (before topk)\n",
    "idx_scores = final_scores[0].clone()\n",
    "idx_scores[idx_scores == float('-inf')] = 0\n",
    "im2 = axes[1].imshow(idx_scores.numpy(), cmap='Oranges', aspect='auto')\n",
    "axes[1].set_title('Indexer Scores\\n(Causal Masked)')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# 3. Sparse selection pattern\n",
    "sparse_pattern = torch.zeros(T, T)\n",
    "for t_pos in range(T):\n",
    "    for j in range(k_effective):\n",
    "        if mask[0, t_pos, j]:\n",
    "            sparse_pattern[t_pos, indices[0, t_pos, j]] = 1.0\n",
    "\n",
    "im3 = axes[2].imshow(sparse_pattern.numpy(), cmap='Greens', aspect='auto')\n",
    "axes[2].set_title(f'Sparse Selection Pattern\\n(k={k_effective} tokens selected)')\n",
    "axes[2].set_xlabel('Key Position')\n",
    "axes[2].set_ylabel('Query Position')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.suptitle('GSA: Full Attention vs Sparse Selection', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sparsity stats\n",
    "total_entries = T * (T + 1) / 2  # causal triangle\n",
    "selected_entries = sparse_pattern.sum().item()\n",
    "print(f\"\\nSparsity: {selected_entries:.0f}/{total_entries:.0f} entries = {selected_entries/total_entries:.1%} selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete GSA Forward Pass\n",
    "\n",
    "| Step | Operation | Shape Transform | Purpose |\n",
    "|------|-----------|----------------|----------|\n",
    "| 1 | q/k/v_proj | [B,T,D] → [B,T,H,d] | Standard attention projections |\n",
    "| 2 | ValueGate (G2) | v = v * σ(W·x+b) | Control value information flow |\n",
    "| 3 | RoPE | Same shape | Positional encoding on Q, K |\n",
    "| 4 | Indexer scoring | [B,T,D] → [B,T,T] | Cheap O(T²) importance scores |\n",
    "| 5 | Top-K selection | [B,T,T] → [B,T,k] indices | Select important tokens |\n",
    "| 6 | Sparse attention | [B,T,H,D] × [B,T,k,H,D] → [B,T,H,D] | Attend only to selected tokens |\n",
    "| 7 | OutputGate (G1) | o = o * σ(W·x+b) | Control output information flow |\n",
    "| 8 | o_proj | [B,T,D] → [B,T,D] | Back to model dim |\n",
    "\n",
    "### Complexity Comparison\n",
    "\n",
    "| | Full Attention | GSA |\n",
    "|---|---|---|\n",
    "| **Indexer** | N/A | O(T² × d_indexer × n_idx_heads) |\n",
    "| **Attention** | O(T² × d_head × H) | O(T × k × d_head × H) |\n",
    "| **Total** | O(T² × D) | O(T² × d_idx × n_idx + T × k × D) |\n",
    "| **Memory** | O(T² × H) | O(T × k × H) |\n",
    "\n",
    "### Why It Works\n",
    "1. **Indexer is cheap** — d_indexer (64) << d_head (128), so T² scoring costs much less\n",
    "2. **Adaptive k** — attend to more tokens when attention is spread, fewer when focused\n",
    "3. **Dual gating** — G2 controls what info enters, G1 controls what info exits\n",
    "4. **Shared indices** — same tokens selected for all heads → efficient gather\n",
    "\n",
    "### GDN vs GSA: Key Differences\n",
    "\n",
    "| | GDN | GSA |\n",
    "|---|---|---|\n",
    "| **Approach** | Recurrent (state matrix) | Sparse attention (token selection) |\n",
    "| **Complexity** | O(T × d_k × d_v) | O(T² × d_idx + T × k × D) |\n",
    "| **Memory** | O(d_k × d_v) constant | O(T × k) linear |\n",
    "| **Parallelism** | Sequential (per token) | Parallelizable (gather + matmul) |\n",
    "| **Best for** | Very long sequences | Moderate sequences with local patterns |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}